{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1FTK8nlEy00LUEnbOkfmxJqrtXrNI_zkd",
      "authorship_tag": "ABX9TyN7DP+uCDMMO3O/69w7WF5b",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aastha12/Deep-Learning/blob/main/CNN/Fashion_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7qwSBOnbDrX"
      },
      "source": [
        "## Create CNN Model and Optimize it using Keras Tuner\n",
        "\n",
        "Make sure to change runtime to GPU otherwise code will take time to execute"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ISrFpm-bHII",
        "outputId": "c3f7358e-872c-400b-b7e0-dd0036a193a8"
      },
      "source": [
        "!pip install keras-tuner"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/ec/1ef246787174b1e2bb591c95f29d3c1310070cad877824f907faba3dade9/keras-tuner-1.0.2.tar.gz (62kB)\n",
            "\r\u001b[K     |█████▏                          | 10kB 23.9MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20kB 29.0MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30kB 32.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 40kB 31.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51kB 28.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 61kB 23.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (20.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.18.5)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.8.7)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (4.41.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->keras-tuner) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->keras-tuner) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->keras-tuner) (0.17.0)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.2-cp36-none-any.whl size=78937 sha256=80a55309251a9380f8cae02ff98b4dcd58d6ed6951d032430a0e87fe85c1e067\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/a1/8a/7c3de0efb3707a1701b36ebbfdbc4e67aedf6d4943a1f463d6\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15356 sha256=d16627ccf23a1da28d3c51d2704808f531dcb3a71d1465964019d593524cbc02\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.4 keras-tuner-1.0.2 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Is_okUzibJDa"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8GWIMxXbsPK"
      },
      "source": [
        "fashion_mnist=keras.datasets.fashion_mnist"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFqj_6PydTS6",
        "outputId": "8082c521-af88-4c64-a0b5-b37be4ad217d"
      },
      "source": [
        "(train_images,train_labels),(test_images,test_labels)=fashion_mnist.load_data()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vr3dy3YmkbFv",
        "outputId": "1a9a5697-2100-4452-c09a-d9c7bc54c29c"
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb9q04_Uk1RF",
        "outputId": "b611b694-32b2-4d3b-c344-3ddaffbdcfde"
      },
      "source": [
        "test_images.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5INJ6x1tk36B"
      },
      "source": [
        "There are 60K images in train set of size 28x28 and 10K images in test size with same size. Let's look at the first element of our train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhEO-0h-kgs1",
        "outputId": "79d0865d-bb00-479d-a242-2db62c472b6b"
      },
      "source": [
        "train_images[0]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
              "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
              "          1,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
              "          0,   3],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
              "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
              "         10,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
              "         72,  15],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
              "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
              "        172,  66],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
              "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
              "        229,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
              "        173,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
              "        202,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
              "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
              "        209,  52],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
              "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
              "        167,  56],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
              "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
              "         92,   0],\n",
              "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
              "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
              "         77,   0],\n",
              "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
              "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
              "        159,   0],\n",
              "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
              "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
              "        215,   0],\n",
              "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
              "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
              "        246,   0],\n",
              "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
              "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
              "        225,   0],\n",
              "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
              "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
              "        229,  29],\n",
              "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
              "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
              "        230,  67],\n",
              "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
              "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
              "        206, 115],\n",
              "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
              "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
              "        210,  92],\n",
              "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
              "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
              "        170,   0],\n",
              "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
              "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEwYbVddkZXS"
      },
      "source": [
        "Each image in the dataset has pixel intensity ranging from 0 to 255, so we normalized images by dividing every pixel value by 255, and now the new scale ranging from 0 to 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h16UHX8ldTxE"
      },
      "source": [
        "train_images=train_images/255\n",
        "test_images=test_images/255"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivniflYXtdNO"
      },
      "source": [
        "#### Reshaping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqzCOwIOlB70"
      },
      "source": [
        "We need to reshape the array in the form (m * n * channels). Since these are gray scale images, our channel will be 1.\n",
        "\n",
        "CNN needs input of the format: number of samples, rows, columns,channels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCA8_-XWfBh8"
      },
      "source": [
        "train_images=train_images.reshape(train_images.shape +(1,))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_TqClJWlfQt",
        "outputId": "4033f9ed-3f49-4463-b14e-30bfdf53f866"
      },
      "source": [
        "train_images.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ihTnHidmODy"
      },
      "source": [
        "test_images=test_images.reshape(test_images.shape +(1,))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IAL6qRu6moTK",
        "outputId": "6fd72a8a-9e32-4772-ae99-efb98fb1303c"
      },
      "source": [
        "test_images.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9jXikxPsvem"
      },
      "source": [
        "#### Keras Tuner\n",
        "\n",
        "##### Early Stopping and Model Checkpoint:\n",
        "\n",
        "Source https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
        "\n",
        "As soon as the loss of the model begins to increase on the test dataset, we will stop training by defining the early stopping callback.\n",
        "\n",
        "You might also noticed that the accuracy of the model starts deteriorating towards the last few epochs. This means that although the performance of the model has improved, we may not have the best performing or most stable model at the end of training. In this case, we are interested in saving the model with the best accuracy on the test dataset. We can address this by using a ModelChecckpoint callback.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0Jmby94FuvF",
        "outputId": "a460338f-52bd-4ce9-971d-e18eb13ea1f5"
      },
      "source": [
        "set(train_labels)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JbfXJUVsv6A"
      },
      "source": [
        "def build_model(hp):\n",
        "  model= keras.Sequential([\n",
        "                                # Convolutional Layer #1\n",
        "                                # Computes __ feature maps(decided by no. of filters) using a mxm filter with ReLU activation.\n",
        "                                # Padding is added to preserve width and height.\n",
        "                                # Input Tensor Shape: [batch_size, 28, 28, 1]\n",
        "                                # Output Tensor Shape: [batch_size, 28, 28, no. of filters]\n",
        "                           keras.layers.Conv2D(\n",
        "                               filters=hp.Int('conv1_filters',min_value=32,max_value=128,step=16), #number of filters in convolution\n",
        "                               kernel_size=hp.Choice('conv1_kernel_size',[3,5]),\n",
        "                               strides=(1,1), \n",
        "                               padding='same',\n",
        "                               kernel_initializer='lecun_normal',\n",
        "                               activation='selu',\n",
        "                               input_shape=(28,28,1)\n",
        "                               ),\n",
        "                                #Adding Pooling layers to reduce overfitting\n",
        "                                # Pooling Layer #1\n",
        "                                # First max pooling layer with a 2x2 filter and stride of 2\n",
        "                                # Input Tensor Shape: [batch_size, 28, 28, no. of filters]\n",
        "                                # Output Tensor Shape: [batch_size, 14, 14, no. of filters]                    \n",
        "                           keras.layers.MaxPool2D(pool_size=(2,2),\n",
        "                                                  strides=(2,2),\n",
        "                                                  padding='same'),\n",
        "\n",
        "                                # Convolutional Layer #2\n",
        "                                # Computes __ feature maps(decided by no. of filters) using a mxm filter with ReLU activation.\n",
        "                                # Padding is added to preserve width and height.\n",
        "                                # Input Tensor Shape: [batch_size, 14, 14, no. of filters]\n",
        "                                # Output Tensor Shape: [batch_size, 14, 14, no. of filters]                         \n",
        "                            keras.layers.Conv2D(\n",
        "                               filters=hp.Int('conv2_filters',min_value=32,max_value=64,step=16), #number of filters in convolution\n",
        "                               kernel_size=hp.Choice('conv2_kernel_size',[3,5]),\n",
        "                               strides=(1,1),\n",
        "                               padding='same',\n",
        "                               kernel_initializer='lecun_normal',\n",
        "                               activation='selu'\n",
        "                               ),    \n",
        "                                #Adding Pooling layers to reduce overfitting\n",
        "                                # Pooling Layer #2\n",
        "                                # First max pooling layer with a 2x2 filter and stride of 2\n",
        "                                # Input Tensor Shape: [batch_size, 14, 14, no. of filters]\n",
        "                                # Output Tensor Shape: [batch_size, 7, 7, no. of filters]                    \n",
        "                           keras.layers.MaxPool2D(pool_size=(2,2),\n",
        "                                                  strides=(2,2),\n",
        "                                                  padding='same'), \n",
        "                           keras.layers.Flatten(),\n",
        "                           keras.layers.Dense(\n",
        "                               units = hp.Int('Dense layer_1',min_value=32,max_value=512,step=16),\n",
        "                               kernel_initializer='lecun_normal',\n",
        "                               activation='selu'\n",
        "                           ),\n",
        "                                # Add dropout operation;  \n",
        "                                # rate=Fraction of the input units to drop.\n",
        "                          keras.layers.Dropout(rate=0.6 ,seed=123),\n",
        "                          keras.layers.Dense(\n",
        "                               units = hp.Int('Dense layer_2',min_value=32,max_value=512,step=16),\n",
        "                               kernel_initializer='lecun_normal',\n",
        "                               activation='selu'\n",
        "                           ),\n",
        "                                # Add dropout operation;  \n",
        "                                # rate=Fraction of the input units to drop.\n",
        "                          keras.layers.Dropout(rate=0.6 ,seed=123),                                               \n",
        "                          keras.layers.Dense(units=10,activation='softmax') #output layer using softmax as we have 10 classes\n",
        "\n",
        "                           \n",
        "  ])\n",
        "\n",
        "  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp.Choice('learning_rate',values=[1e-2, 1e-3])),\n",
        "                #since we have 10 integer classes use, sparse_categorical_crossentropy\n",
        "                #read more here: https://keras.io/api/losses/probabilistic_losses/#sparse_categorical_crossentropy-function\n",
        "                #for binary classes, use binary_crossentropy\n",
        "                #for one hot encoded multiple classes, use categorical_crossentropy\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy']\n",
        "                )\n",
        "  return model"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_ngVTv_3SnA"
      },
      "source": [
        "from kerastuner import BayesianOptimization"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9tgGvnl576cs",
        "outputId": "eeb916c0-acc8-45ca-d3b4-c477960d5067"
      },
      "source": [
        "tuner_search=BayesianOptimization( build_model,\n",
        "                          objective='val_accuracy',\n",
        "                          max_trials=5,directory='output',project_name=\"MNIST Fashion\", seed=123)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project output/MNIST Fashion/oracle.json\n",
            "INFO:tensorflow:Reloading Tuner from output/MNIST Fashion/tuner0.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anU4-W_cv2Sn"
      },
      "source": [
        "We will first use 5 epochs to get the best model and then train our best model on 100 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOZ3iin08Dow",
        "outputId": "283d41aa-80a3-4d48-e7ee-40a874777c7e"
      },
      "source": [
        "tuner_search.search(train_images,train_labels,epochs=5,validation_split=0.1 )"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ruuUz2yJEkCB",
        "outputId": "b1b5e8b7-fbb9-4280-e9f8-cf7df76ddee1"
      },
      "source": [
        "tuner_search.search_space_summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Search space summary\n",
            "Default search space size: 7\n",
            "conv1_filters (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 128, 'step': 16, 'sampling': None}\n",
            "conv1_kernel_size (Choice)\n",
            "{'default': 3, 'conditions': [], 'values': [3, 5], 'ordered': True}\n",
            "conv2_filters (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 64, 'step': 16, 'sampling': None}\n",
            "conv2_kernel_size (Choice)\n",
            "{'default': 3, 'conditions': [], 'values': [3, 5], 'ordered': True}\n",
            "Dense layer_1 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 16, 'sampling': None}\n",
            "Dense layer_2 (Int)\n",
            "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 512, 'step': 16, 'sampling': None}\n",
            "learning_rate (Choice)\n",
            "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001], 'ordered': True}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptOybvpX9GJM"
      },
      "source": [
        "model=tuner_search.get_best_models(num_models=1)[0]"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGBHUtQNwFnh",
        "outputId": "958791bf-e2f3-44ee-d6bb-dd6e527d2ee2"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 96)        960       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 14, 14, 96)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 14, 14, 32)        27680     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1568)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 448)               702912    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 448)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 208)               93392     \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 208)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                2090      \n",
            "=================================================================\n",
            "Total params: 827,034\n",
            "Trainable params: 827,034\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B9i1rIWa_6Ey",
        "outputId": "7d9e36ec-15a5-4a30-b2f1-f2aa2c9a7f51"
      },
      "source": [
        "tuner_search.results_summary()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Results summary\n",
            "Results in output/MNIST Fashion\n",
            "Showing 10 best trials\n",
            "Objective(name='val_accuracy', direction='max')\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "conv1_filters: 96\n",
            "conv1_kernel_size: 3\n",
            "conv2_filters: 32\n",
            "conv2_kernel_size: 3\n",
            "Dense layer_1: 448\n",
            "Dense layer_2: 208\n",
            "learning_rate: 0.001\n",
            "Score: 0.9048333168029785\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "conv1_filters: 32\n",
            "conv1_kernel_size: 3\n",
            "conv2_filters: 32\n",
            "conv2_kernel_size: 5\n",
            "Dense layer_1: 32\n",
            "Dense layer_2: 512\n",
            "learning_rate: 0.001\n",
            "Score: 0.8973333239555359\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "conv1_filters: 32\n",
            "conv1_kernel_size: 5\n",
            "conv2_filters: 64\n",
            "conv2_kernel_size: 3\n",
            "Dense layer_1: 48\n",
            "Dense layer_2: 480\n",
            "learning_rate: 0.001\n",
            "Score: 0.8961666822433472\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "conv1_filters: 80\n",
            "conv1_kernel_size: 5\n",
            "conv2_filters: 48\n",
            "conv2_kernel_size: 5\n",
            "Dense layer_1: 272\n",
            "Dense layer_2: 352\n",
            "learning_rate: 0.001\n",
            "Score: 0.8961666822433472\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "conv1_filters: 128\n",
            "conv1_kernel_size: 5\n",
            "conv2_filters: 48\n",
            "conv2_kernel_size: 5\n",
            "Dense layer_1: 400\n",
            "Dense layer_2: 80\n",
            "learning_rate: 0.01\n",
            "Score: 0.1054999977350235\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOv5h9dCwG8q",
        "outputId": "64aea839-a125-446f-fbbe-e40aeba9b81a"
      },
      "source": [
        "model_history=model.fit(train_images,train_labels,epochs=100,validation_split=0.1,initial_epoch=3)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 4/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3522 - accuracy: 0.8810 - val_loss: 0.2866 - val_accuracy: 0.8945\n",
            "Epoch 5/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3530 - accuracy: 0.8828 - val_loss: 0.2858 - val_accuracy: 0.9032\n",
            "Epoch 6/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3386 - accuracy: 0.8870 - val_loss: 0.2790 - val_accuracy: 0.9035\n",
            "Epoch 7/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3359 - accuracy: 0.8872 - val_loss: 0.2714 - val_accuracy: 0.9060\n",
            "Epoch 8/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3260 - accuracy: 0.8923 - val_loss: 0.3038 - val_accuracy: 0.9062\n",
            "Epoch 9/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3216 - accuracy: 0.8926 - val_loss: 0.2770 - val_accuracy: 0.9127\n",
            "Epoch 10/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3113 - accuracy: 0.8971 - val_loss: 0.2853 - val_accuracy: 0.9113\n",
            "Epoch 11/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3043 - accuracy: 0.9001 - val_loss: 0.3074 - val_accuracy: 0.9057\n",
            "Epoch 12/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2967 - accuracy: 0.9030 - val_loss: 0.2963 - val_accuracy: 0.9105\n",
            "Epoch 13/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.3018 - accuracy: 0.9036 - val_loss: 0.2899 - val_accuracy: 0.9133\n",
            "Epoch 14/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2948 - accuracy: 0.9053 - val_loss: 0.3025 - val_accuracy: 0.9130\n",
            "Epoch 15/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2924 - accuracy: 0.9062 - val_loss: 0.2820 - val_accuracy: 0.9057\n",
            "Epoch 16/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2862 - accuracy: 0.9081 - val_loss: 0.3191 - val_accuracy: 0.9080\n",
            "Epoch 17/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2769 - accuracy: 0.9107 - val_loss: 0.3024 - val_accuracy: 0.9188\n",
            "Epoch 18/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2791 - accuracy: 0.9105 - val_loss: 0.3303 - val_accuracy: 0.9047\n",
            "Epoch 19/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2815 - accuracy: 0.9103 - val_loss: 0.3508 - val_accuracy: 0.9115\n",
            "Epoch 20/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2718 - accuracy: 0.9141 - val_loss: 0.3174 - val_accuracy: 0.9103\n",
            "Epoch 21/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2676 - accuracy: 0.9144 - val_loss: 0.3502 - val_accuracy: 0.9052\n",
            "Epoch 22/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2762 - accuracy: 0.9146 - val_loss: 0.3366 - val_accuracy: 0.9132\n",
            "Epoch 23/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2695 - accuracy: 0.9152 - val_loss: 0.3691 - val_accuracy: 0.9045\n",
            "Epoch 24/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2630 - accuracy: 0.9169 - val_loss: 0.3476 - val_accuracy: 0.9175\n",
            "Epoch 25/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2726 - accuracy: 0.9171 - val_loss: 0.3643 - val_accuracy: 0.9108\n",
            "Epoch 26/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2687 - accuracy: 0.9170 - val_loss: 0.3785 - val_accuracy: 0.9035\n",
            "Epoch 27/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2566 - accuracy: 0.9190 - val_loss: 0.3163 - val_accuracy: 0.9197\n",
            "Epoch 28/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2545 - accuracy: 0.9192 - val_loss: 0.4289 - val_accuracy: 0.9140\n",
            "Epoch 29/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2594 - accuracy: 0.9189 - val_loss: 0.4077 - val_accuracy: 0.9153\n",
            "Epoch 30/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2562 - accuracy: 0.9209 - val_loss: 0.3274 - val_accuracy: 0.9128\n",
            "Epoch 31/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2477 - accuracy: 0.9229 - val_loss: 0.4569 - val_accuracy: 0.8993\n",
            "Epoch 32/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2461 - accuracy: 0.9240 - val_loss: 0.4105 - val_accuracy: 0.9123\n",
            "Epoch 33/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2549 - accuracy: 0.9232 - val_loss: 0.4393 - val_accuracy: 0.9127\n",
            "Epoch 34/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2489 - accuracy: 0.9249 - val_loss: 0.3517 - val_accuracy: 0.9187\n",
            "Epoch 35/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2424 - accuracy: 0.9247 - val_loss: 0.4771 - val_accuracy: 0.9097\n",
            "Epoch 36/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2468 - accuracy: 0.9252 - val_loss: 0.3976 - val_accuracy: 0.9135\n",
            "Epoch 37/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2430 - accuracy: 0.9271 - val_loss: 0.3775 - val_accuracy: 0.9162\n",
            "Epoch 38/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2487 - accuracy: 0.9249 - val_loss: 0.4493 - val_accuracy: 0.9065\n",
            "Epoch 39/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2366 - accuracy: 0.9281 - val_loss: 0.4688 - val_accuracy: 0.9165\n",
            "Epoch 40/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2370 - accuracy: 0.9287 - val_loss: 0.4765 - val_accuracy: 0.8928\n",
            "Epoch 41/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2367 - accuracy: 0.9296 - val_loss: 0.4442 - val_accuracy: 0.9053\n",
            "Epoch 42/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2649 - accuracy: 0.9260 - val_loss: 0.4953 - val_accuracy: 0.9132\n",
            "Epoch 43/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2395 - accuracy: 0.9281 - val_loss: 0.4458 - val_accuracy: 0.9150\n",
            "Epoch 44/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2484 - accuracy: 0.9281 - val_loss: 0.4823 - val_accuracy: 0.9178\n",
            "Epoch 45/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2249 - accuracy: 0.9309 - val_loss: 0.5153 - val_accuracy: 0.9213\n",
            "Epoch 46/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2299 - accuracy: 0.9315 - val_loss: 0.5252 - val_accuracy: 0.9058\n",
            "Epoch 47/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2398 - accuracy: 0.9282 - val_loss: 0.6168 - val_accuracy: 0.9163\n",
            "Epoch 48/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2438 - accuracy: 0.9294 - val_loss: 0.4976 - val_accuracy: 0.9163\n",
            "Epoch 49/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2358 - accuracy: 0.9297 - val_loss: 0.4687 - val_accuracy: 0.9168\n",
            "Epoch 50/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2388 - accuracy: 0.9310 - val_loss: 0.5039 - val_accuracy: 0.9168\n",
            "Epoch 51/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2305 - accuracy: 0.9304 - val_loss: 0.4931 - val_accuracy: 0.9202\n",
            "Epoch 52/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2224 - accuracy: 0.9333 - val_loss: 0.5317 - val_accuracy: 0.9085\n",
            "Epoch 53/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2427 - accuracy: 0.9296 - val_loss: 0.5087 - val_accuracy: 0.9193\n",
            "Epoch 54/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2221 - accuracy: 0.9349 - val_loss: 0.5634 - val_accuracy: 0.9113\n",
            "Epoch 55/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2438 - accuracy: 0.9292 - val_loss: 0.4374 - val_accuracy: 0.9188\n",
            "Epoch 56/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2484 - accuracy: 0.9310 - val_loss: 0.5147 - val_accuracy: 0.9155\n",
            "Epoch 57/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2045 - accuracy: 0.9378 - val_loss: 0.7333 - val_accuracy: 0.9167\n",
            "Epoch 58/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2442 - accuracy: 0.9310 - val_loss: 0.5861 - val_accuracy: 0.9173\n",
            "Epoch 59/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2313 - accuracy: 0.9339 - val_loss: 0.7736 - val_accuracy: 0.9140\n",
            "Epoch 60/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2392 - accuracy: 0.9318 - val_loss: 0.6272 - val_accuracy: 0.9105\n",
            "Epoch 61/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2345 - accuracy: 0.9340 - val_loss: 0.5430 - val_accuracy: 0.9103\n",
            "Epoch 62/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2312 - accuracy: 0.9336 - val_loss: 0.6727 - val_accuracy: 0.9157\n",
            "Epoch 63/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2092 - accuracy: 0.9365 - val_loss: 0.6742 - val_accuracy: 0.9205\n",
            "Epoch 64/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2234 - accuracy: 0.9357 - val_loss: 0.6504 - val_accuracy: 0.9175\n",
            "Epoch 65/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2486 - accuracy: 0.9327 - val_loss: 0.8945 - val_accuracy: 0.9143\n",
            "Epoch 66/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2321 - accuracy: 0.9349 - val_loss: 0.6113 - val_accuracy: 0.9118\n",
            "Epoch 67/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2469 - accuracy: 0.9313 - val_loss: 0.5267 - val_accuracy: 0.9178\n",
            "Epoch 68/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2226 - accuracy: 0.9360 - val_loss: 0.5695 - val_accuracy: 0.9083\n",
            "Epoch 69/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2212 - accuracy: 0.9369 - val_loss: 0.5803 - val_accuracy: 0.9105\n",
            "Epoch 70/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2702 - accuracy: 0.9294 - val_loss: 0.5533 - val_accuracy: 0.9142\n",
            "Epoch 71/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2386 - accuracy: 0.9347 - val_loss: 0.7818 - val_accuracy: 0.9170\n",
            "Epoch 72/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2250 - accuracy: 0.9373 - val_loss: 0.7771 - val_accuracy: 0.9092\n",
            "Epoch 73/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2350 - accuracy: 0.9364 - val_loss: 0.6906 - val_accuracy: 0.9092\n",
            "Epoch 74/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2281 - accuracy: 0.9341 - val_loss: 0.7517 - val_accuracy: 0.9173\n",
            "Epoch 75/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2066 - accuracy: 0.9397 - val_loss: 0.7752 - val_accuracy: 0.9202\n",
            "Epoch 76/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2772 - accuracy: 0.9271 - val_loss: 0.5063 - val_accuracy: 0.9115\n",
            "Epoch 77/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2141 - accuracy: 0.9389 - val_loss: 0.7523 - val_accuracy: 0.9183\n",
            "Epoch 78/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2090 - accuracy: 0.9394 - val_loss: 0.6561 - val_accuracy: 0.9128\n",
            "Epoch 79/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2515 - accuracy: 0.9355 - val_loss: 0.9279 - val_accuracy: 0.9157\n",
            "Epoch 80/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2311 - accuracy: 0.9354 - val_loss: 0.9316 - val_accuracy: 0.9118\n",
            "Epoch 81/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2538 - accuracy: 0.9338 - val_loss: 0.8226 - val_accuracy: 0.9217\n",
            "Epoch 82/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2447 - accuracy: 0.9331 - val_loss: 0.8350 - val_accuracy: 0.9182\n",
            "Epoch 83/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2367 - accuracy: 0.9383 - val_loss: 0.7465 - val_accuracy: 0.9148\n",
            "Epoch 84/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2360 - accuracy: 0.9364 - val_loss: 0.7981 - val_accuracy: 0.9175\n",
            "Epoch 85/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2439 - accuracy: 0.9336 - val_loss: 0.6502 - val_accuracy: 0.9063\n",
            "Epoch 86/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2240 - accuracy: 0.9397 - val_loss: 0.9096 - val_accuracy: 0.9182\n",
            "Epoch 87/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2333 - accuracy: 0.9369 - val_loss: 0.8605 - val_accuracy: 0.9147\n",
            "Epoch 88/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2318 - accuracy: 0.9371 - val_loss: 0.9343 - val_accuracy: 0.9157\n",
            "Epoch 89/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2513 - accuracy: 0.9324 - val_loss: 0.6352 - val_accuracy: 0.9115\n",
            "Epoch 90/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2211 - accuracy: 0.9390 - val_loss: 0.6982 - val_accuracy: 0.9017\n",
            "Epoch 91/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2495 - accuracy: 0.9369 - val_loss: 0.6227 - val_accuracy: 0.9165\n",
            "Epoch 92/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2466 - accuracy: 0.9334 - val_loss: 0.8041 - val_accuracy: 0.9023\n",
            "Epoch 93/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2413 - accuracy: 0.9343 - val_loss: 0.6465 - val_accuracy: 0.9150\n",
            "Epoch 94/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2282 - accuracy: 0.9351 - val_loss: 0.7010 - val_accuracy: 0.9152\n",
            "Epoch 95/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2447 - accuracy: 0.9329 - val_loss: 0.6759 - val_accuracy: 0.9153\n",
            "Epoch 96/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2562 - accuracy: 0.9311 - val_loss: 0.7028 - val_accuracy: 0.9195\n",
            "Epoch 97/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2370 - accuracy: 0.9357 - val_loss: 0.7844 - val_accuracy: 0.9153\n",
            "Epoch 98/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2238 - accuracy: 0.9389 - val_loss: 0.8175 - val_accuracy: 0.9177\n",
            "Epoch 99/100\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2354 - accuracy: 0.9352 - val_loss: 0.7858 - val_accuracy: 0.9168\n",
            "Epoch 100/100\n",
            "1688/1688 [==============================] - 6s 3ms/step - loss: 0.2415 - accuracy: 0.9346 - val_loss: 0.8325 - val_accuracy: 0.9140\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wGfcTJtxJM_",
        "outputId": "876ed76a-30de-4116-99d6-2fb4c3172a3a"
      },
      "source": [
        "print(model_history.history.keys())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "hqleO-pv6DkT",
        "outputId": "02585c13-02e9-4de8-9fac-1a2a389453f5"
      },
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig=go.Figure()\n",
        "\n",
        "x=np.linspace(4, 100,endpoint=True)\n",
        "\n",
        "fig.add_trace(go.Scatter(x=x, y=model_history.history['accuracy'],\n",
        "                    mode='lines',\n",
        "                    name='Training accuracy'))\n",
        "fig.add_trace(go.Scatter(x=x, y=model_history.history['val_accuracy'],\n",
        "                    mode='lines',\n",
        "                    name='Validation accuracy'))\n",
        "fig.add_trace(go.Scatter(x=x, y=model_history.history['loss'],\n",
        "                    mode='lines',\n",
        "                    name='Training loss'))\n",
        "fig.add_trace(go.Scatter(x=x, y=model_history.history['val_loss'],\n",
        "                    mode='lines',\n",
        "                    name='Validation loss'))\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"dfc80258-5399-4c25-b5e6-47c1eb61a3dd\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"dfc80258-5399-4c25-b5e6-47c1eb61a3dd\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'dfc80258-5399-4c25-b5e6-47c1eb61a3dd',\n",
              "                        [{\"mode\": \"lines\", \"name\": \"Training accuracy\", \"type\": \"scatter\", \"x\": [4.0, 5.959183673469388, 7.918367346938775, 9.877551020408163, 11.83673469387755, 13.795918367346939, 15.755102040816325, 17.714285714285715, 19.6734693877551, 21.632653061224488, 23.591836734693878, 25.551020408163264, 27.51020408163265, 29.46938775510204, 31.428571428571427, 33.38775510204081, 35.3469387755102, 37.30612244897959, 39.265306122448976, 41.224489795918366, 43.183673469387756, 45.14285714285714, 47.10204081632653, 49.06122448979592, 51.0204081632653, 52.97959183673469, 54.93877551020408, 56.89795918367347, 58.857142857142854, 60.816326530612244, 62.775510204081634, 64.73469387755102, 66.6938775510204, 68.65306122448979, 70.61224489795919, 72.57142857142857, 74.53061224489795, 76.48979591836735, 78.44897959183673, 80.40816326530611, 82.36734693877551, 84.3265306122449, 86.28571428571428, 88.24489795918367, 90.20408163265306, 92.16326530612244, 94.12244897959184, 96.08163265306122, 98.0408163265306, 100.0], \"y\": [0.8810370564460754, 0.8827962875366211, 0.8870370388031006, 0.8871666789054871, 0.8922592401504517, 0.8926110863685608, 0.8970741033554077, 0.9001481533050537, 0.9030185341835022, 0.903592586517334, 0.9053333401679993, 0.9062407612800598, 0.9081296324729919, 0.9106851816177368, 0.91053706407547, 0.9103333353996277, 0.9140555262565613, 0.9144259095191956, 0.9145740866661072, 0.9152222275733948, 0.9168518781661987, 0.9170926213264465, 0.9170185327529907, 0.9190185070037842, 0.9192036986351013, 0.9189074039459229, 0.9208518266677856, 0.9228518605232239, 0.9239814877510071, 0.923203706741333, 0.924907386302948, 0.9247221946716309, 0.9251852035522461, 0.9270740747451782, 0.9248889088630676, 0.9280925989151001, 0.9287222027778625, 0.9295555353164673, 0.9260370135307312, 0.9281111359596252, 0.9280925989151001, 0.9309074282646179, 0.9315370321273804, 0.9282037019729614, 0.9293703436851501, 0.9296851754188538, 0.9310370087623596, 0.9303703904151917, 0.9333333373069763, 0.9296481609344482, 0.9348518252372742, 0.9292407631874084, 0.9310185313224792, 0.9378148317337036, 0.930981457233429, 0.9339259266853333, 0.9318333268165588, 0.9340370297431946, 0.9335926175117493, 0.9365370273590088, 0.9356851577758789, 0.9327037334442139, 0.9348888993263245, 0.9312962889671326, 0.9359999895095825, 0.9368888735771179, 0.9294259548187256, 0.9347407221794128, 0.937333345413208, 0.9363889098167419, 0.9341296553611755, 0.9396851658821106, 0.9270926117897034, 0.9389259219169617, 0.9393518567085266, 0.9354814887046814, 0.9353518486022949, 0.9338333606719971, 0.9331111311912537, 0.9382963180541992, 0.9364444613456726, 0.933555543422699, 0.9396666884422302, 0.9369074106216431, 0.9370555281639099, 0.9323703646659851, 0.9389629364013672, 0.9368703961372375, 0.933388888835907, 0.9342592358589172, 0.935092568397522, 0.9328518509864807, 0.9310555458068848, 0.9357407689094543, 0.9389073848724365, 0.9352222084999084, 0.934592604637146]}, {\"mode\": \"lines\", \"name\": \"Validation accuracy\", \"type\": \"scatter\", \"x\": [4.0, 5.959183673469388, 7.918367346938775, 9.877551020408163, 11.83673469387755, 13.795918367346939, 15.755102040816325, 17.714285714285715, 19.6734693877551, 21.632653061224488, 23.591836734693878, 25.551020408163264, 27.51020408163265, 29.46938775510204, 31.428571428571427, 33.38775510204081, 35.3469387755102, 37.30612244897959, 39.265306122448976, 41.224489795918366, 43.183673469387756, 45.14285714285714, 47.10204081632653, 49.06122448979592, 51.0204081632653, 52.97959183673469, 54.93877551020408, 56.89795918367347, 58.857142857142854, 60.816326530612244, 62.775510204081634, 64.73469387755102, 66.6938775510204, 68.65306122448979, 70.61224489795919, 72.57142857142857, 74.53061224489795, 76.48979591836735, 78.44897959183673, 80.40816326530611, 82.36734693877551, 84.3265306122449, 86.28571428571428, 88.24489795918367, 90.20408163265306, 92.16326530612244, 94.12244897959184, 96.08163265306122, 98.0408163265306, 100.0], \"y\": [0.8945000171661377, 0.903166651725769, 0.9035000205039978, 0.906000018119812, 0.906166672706604, 0.9126666784286499, 0.9113333225250244, 0.9056666493415833, 0.9104999899864197, 0.9133333563804626, 0.9129999876022339, 0.9056666493415833, 0.9079999923706055, 0.918833315372467, 0.9046666622161865, 0.9114999771118164, 0.9103333353996277, 0.9051666855812073, 0.9131666421890259, 0.9045000076293945, 0.9175000190734863, 0.9108333587646484, 0.9035000205039978, 0.9196666479110718, 0.9139999747276306, 0.9153333306312561, 0.9128333330154419, 0.8993333578109741, 0.9123333096504211, 0.9126666784286499, 0.918666660785675, 0.9096666574478149, 0.9135000109672546, 0.9161666631698608, 0.906499981880188, 0.9164999723434448, 0.8928333520889282, 0.9053333401679993, 0.9131666421890259, 0.9150000214576721, 0.9178333282470703, 0.9213333129882812, 0.9058333039283752, 0.9163333177566528, 0.9163333177566528, 0.9168333411216736, 0.9168333411216736, 0.9201666712760925, 0.9085000157356262, 0.9193333387374878, 0.9113333225250244, 0.918833315372467, 0.9154999852180481, 0.9166666865348816, 0.9173333048820496, 0.9139999747276306, 0.9104999899864197, 0.9103333353996277, 0.9156666398048401, 0.9204999804496765, 0.9175000190734863, 0.9143333435058594, 0.9118333458900452, 0.9178333282470703, 0.9083333611488342, 0.9104999899864197, 0.9141666889190674, 0.9169999957084656, 0.909166693687439, 0.909166693687439, 0.9173333048820496, 0.9201666712760925, 0.9114999771118164, 0.9183333516120911, 0.9128333330154419, 0.9156666398048401, 0.9118333458900452, 0.92166668176651, 0.9181666374206543, 0.9148333072662354, 0.9175000190734863, 0.906333327293396, 0.9181666374206543, 0.9146666526794434, 0.9156666398048401, 0.9114999771118164, 0.9016666412353516, 0.9164999723434448, 0.9023333191871643, 0.9150000214576721, 0.9151666760444641, 0.9153333306312561, 0.9194999933242798, 0.9153333306312561, 0.9176666736602783, 0.9168333411216736, 0.9139999747276306]}, {\"mode\": \"lines\", \"name\": \"Training loss\", \"type\": \"scatter\", \"x\": [4.0, 5.959183673469388, 7.918367346938775, 9.877551020408163, 11.83673469387755, 13.795918367346939, 15.755102040816325, 17.714285714285715, 19.6734693877551, 21.632653061224488, 23.591836734693878, 25.551020408163264, 27.51020408163265, 29.46938775510204, 31.428571428571427, 33.38775510204081, 35.3469387755102, 37.30612244897959, 39.265306122448976, 41.224489795918366, 43.183673469387756, 45.14285714285714, 47.10204081632653, 49.06122448979592, 51.0204081632653, 52.97959183673469, 54.93877551020408, 56.89795918367347, 58.857142857142854, 60.816326530612244, 62.775510204081634, 64.73469387755102, 66.6938775510204, 68.65306122448979, 70.61224489795919, 72.57142857142857, 74.53061224489795, 76.48979591836735, 78.44897959183673, 80.40816326530611, 82.36734693877551, 84.3265306122449, 86.28571428571428, 88.24489795918367, 90.20408163265306, 92.16326530612244, 94.12244897959184, 96.08163265306122, 98.0408163265306, 100.0], \"y\": [0.35224804282188416, 0.3529609441757202, 0.33864015340805054, 0.33585143089294434, 0.3260059058666229, 0.3216223120689392, 0.3112815320491791, 0.30426451563835144, 0.2967054545879364, 0.30175334215164185, 0.29475879669189453, 0.29243573546409607, 0.2861512005329132, 0.27686676383018494, 0.2791065573692322, 0.281538188457489, 0.2718196213245392, 0.26755669713020325, 0.2762272357940674, 0.2694767117500305, 0.2630176246166229, 0.27257397770881653, 0.26873886585235596, 0.25656500458717346, 0.2544896602630615, 0.25939708948135376, 0.2561897933483124, 0.2476678490638733, 0.24606753885746002, 0.2548726797103882, 0.24888883531093597, 0.24239970743656158, 0.2468014806509018, 0.24303598701953888, 0.24869006872177124, 0.2366189807653427, 0.23700474202632904, 0.23667578399181366, 0.2648773789405823, 0.23946236073970795, 0.24841926991939545, 0.2248956710100174, 0.22991836071014404, 0.2397763431072235, 0.24382449686527252, 0.23578256368637085, 0.23875173926353455, 0.2304644137620926, 0.22238340973854065, 0.24267008900642395, 0.22205935418605804, 0.2438039630651474, 0.248435840010643, 0.20446504652500153, 0.24419105052947998, 0.23131000995635986, 0.23920956254005432, 0.23449450731277466, 0.2311987429857254, 0.20915544033050537, 0.2233646810054779, 0.24860109388828278, 0.23207905888557434, 0.24687820672988892, 0.22258634865283966, 0.22117406129837036, 0.2701791226863861, 0.23855212330818176, 0.22500087320804596, 0.23502005636692047, 0.2281111180782318, 0.2065681666135788, 0.27717825770378113, 0.2140861302614212, 0.20896729826927185, 0.25152039527893066, 0.23109422624111176, 0.25378158688545227, 0.2447214275598526, 0.23665007948875427, 0.2360292226076126, 0.24387525022029877, 0.22399742901325226, 0.23328037559986115, 0.23180489242076874, 0.2512889802455902, 0.22112707793712616, 0.24948923289775848, 0.24657116830348969, 0.24133120477199554, 0.22817838191986084, 0.24470582604408264, 0.2562086284160614, 0.23702692985534668, 0.22375673055648804, 0.23544827103614807, 0.24148108065128326]}, {\"mode\": \"lines\", \"name\": \"Validation loss\", \"type\": \"scatter\", \"x\": [4.0, 5.959183673469388, 7.918367346938775, 9.877551020408163, 11.83673469387755, 13.795918367346939, 15.755102040816325, 17.714285714285715, 19.6734693877551, 21.632653061224488, 23.591836734693878, 25.551020408163264, 27.51020408163265, 29.46938775510204, 31.428571428571427, 33.38775510204081, 35.3469387755102, 37.30612244897959, 39.265306122448976, 41.224489795918366, 43.183673469387756, 45.14285714285714, 47.10204081632653, 49.06122448979592, 51.0204081632653, 52.97959183673469, 54.93877551020408, 56.89795918367347, 58.857142857142854, 60.816326530612244, 62.775510204081634, 64.73469387755102, 66.6938775510204, 68.65306122448979, 70.61224489795919, 72.57142857142857, 74.53061224489795, 76.48979591836735, 78.44897959183673, 80.40816326530611, 82.36734693877551, 84.3265306122449, 86.28571428571428, 88.24489795918367, 90.20408163265306, 92.16326530612244, 94.12244897959184, 96.08163265306122, 98.0408163265306, 100.0], \"y\": [0.2865574061870575, 0.285759836435318, 0.27898770570755005, 0.2714039385318756, 0.3037559688091278, 0.27696242928504944, 0.28533849120140076, 0.3073789179325104, 0.29625871777534485, 0.2898644506931305, 0.3024979531764984, 0.28199344873428345, 0.31913992762565613, 0.3023892045021057, 0.330342173576355, 0.35084548592567444, 0.31744515895843506, 0.3501541316509247, 0.336597204208374, 0.36905619502067566, 0.34755074977874756, 0.3642716407775879, 0.37850674986839294, 0.31627196073532104, 0.42894718050956726, 0.40769466757774353, 0.3274449408054352, 0.4569268822669983, 0.41049665212631226, 0.4392945170402527, 0.351728230714798, 0.47705700993537903, 0.39759477972984314, 0.3775460124015808, 0.44934985041618347, 0.4688318967819214, 0.4764678180217743, 0.44419345259666443, 0.4953209161758423, 0.44580936431884766, 0.48230797052383423, 0.515289306640625, 0.5251607894897461, 0.6167541742324829, 0.49761962890625, 0.4686727821826935, 0.5038737654685974, 0.4931287467479706, 0.5317227244377136, 0.5086897015571594, 0.5633854866027832, 0.43736201524734497, 0.5147047638893127, 0.733292818069458, 0.5861064195632935, 0.7736048698425293, 0.6272062063217163, 0.5429993271827698, 0.6726754307746887, 0.6741881370544434, 0.650399386882782, 0.8945492506027222, 0.6112726330757141, 0.5267096757888794, 0.5694509744644165, 0.5802506804466248, 0.5533424615859985, 0.7818173766136169, 0.7770561575889587, 0.6906241774559021, 0.75174480676651, 0.7751961350440979, 0.506255030632019, 0.7522825002670288, 0.6561320424079895, 0.9278651475906372, 0.9315824508666992, 0.8226076364517212, 0.8350092172622681, 0.7465419173240662, 0.7981364130973816, 0.6501817107200623, 0.9095998406410217, 0.860529363155365, 0.9342590570449829, 0.6351824402809143, 0.6982344388961792, 0.6226790547370911, 0.8040945529937744, 0.6465011835098267, 0.700997531414032, 0.6758669018745422, 0.7028068900108337, 0.7843685150146484, 0.8175077438354492, 0.7858022451400757, 0.8325449824333191]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('dfc80258-5399-4c25-b5e6-47c1eb61a3dd');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px16WjjOwwGG"
      },
      "source": [
        "#### Adding Early Stopping and ModelCheckpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1K440Bb8m_h",
        "outputId": "ab291abc-736e-4bcd-c470-ce07dd06f512"
      },
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "\n",
        "# simple early stopping\n",
        "\"\"\"\n",
        "Source: https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n",
        "\n",
        "“patience” argument:\n",
        "In this case, we will wait 200 epochs before training is stopped. \n",
        "This means that we will allow training to continue for up to an additional \n",
        "200 epochs after the point that validation loss started to degrade, giving \n",
        "the training process an opportunity to get across flat spots or find some \n",
        "additional improvement.\n",
        "\"\"\"\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=200)\n",
        "\n",
        "\"\"\"\n",
        "we will use accuracy on the validation in the ModelCheckpoint callback \n",
        "to save the best model observed during training\n",
        "We could also seek the model with the best loss on the test dataset, \n",
        "but this may or may not correspond to the model with the best accuracy.\n",
        "\"\"\"\n",
        "mc = ModelCheckpoint('/content/best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=False)\n",
        "\n",
        "#get best model from kerastuner\n",
        "model=tuner_search.get_best_models(num_models=1)[0]\n",
        "\n",
        "# fit model\n",
        "model_history=model.fit(train_images,train_labels,epochs=4000,validation_split=0.1,initial_epoch=3, callbacks=[es, mc])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "Epoch 4/4000\n",
            "1682/1688 [============================>.] - ETA: 0s - loss: 0.3602 - accuracy: 0.8786\n",
            "Epoch 00004: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3605 - accuracy: 0.8786 - val_loss: 0.2916 - val_accuracy: 0.8995\n",
            "Epoch 5/4000\n",
            "1680/1688 [============================>.] - ETA: 0s - loss: 0.3408 - accuracy: 0.8856\n",
            "Epoch 00005: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3413 - accuracy: 0.8855 - val_loss: 0.2921 - val_accuracy: 0.9043\n",
            "Epoch 6/4000\n",
            "1674/1688 [============================>.] - ETA: 0s - loss: 0.3341 - accuracy: 0.8893\n",
            "Epoch 00006: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3344 - accuracy: 0.8892 - val_loss: 0.2650 - val_accuracy: 0.9058\n",
            "Epoch 7/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.3257 - accuracy: 0.8922\n",
            "Epoch 00007: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3255 - accuracy: 0.8923 - val_loss: 0.3106 - val_accuracy: 0.9015\n",
            "Epoch 8/4000\n",
            "1678/1688 [============================>.] - ETA: 0s - loss: 0.3240 - accuracy: 0.8936\n",
            "Epoch 00008: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3241 - accuracy: 0.8936 - val_loss: 0.2897 - val_accuracy: 0.9002\n",
            "Epoch 9/4000\n",
            "1675/1688 [============================>.] - ETA: 0s - loss: 0.3171 - accuracy: 0.8957\n",
            "Epoch 00009: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3170 - accuracy: 0.8956 - val_loss: 0.2679 - val_accuracy: 0.9128\n",
            "Epoch 10/4000\n",
            "1682/1688 [============================>.] - ETA: 0s - loss: 0.3110 - accuracy: 0.8979\n",
            "Epoch 00010: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3110 - accuracy: 0.8979 - val_loss: 0.2885 - val_accuracy: 0.9053\n",
            "Epoch 11/4000\n",
            "1683/1688 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.8981\n",
            "Epoch 00011: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3085 - accuracy: 0.8981 - val_loss: 0.3031 - val_accuracy: 0.9028\n",
            "Epoch 12/4000\n",
            "1682/1688 [============================>.] - ETA: 0s - loss: 0.3054 - accuracy: 0.9012\n",
            "Epoch 00012: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3059 - accuracy: 0.9011 - val_loss: 0.2938 - val_accuracy: 0.9030\n",
            "Epoch 13/4000\n",
            "1682/1688 [============================>.] - ETA: 0s - loss: 0.2934 - accuracy: 0.9042\n",
            "Epoch 00013: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2932 - accuracy: 0.9041 - val_loss: 0.3429 - val_accuracy: 0.9097\n",
            "Epoch 14/4000\n",
            "1677/1688 [============================>.] - ETA: 0s - loss: 0.2922 - accuracy: 0.9060\n",
            "Epoch 00014: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2922 - accuracy: 0.9060 - val_loss: 0.3035 - val_accuracy: 0.9067\n",
            "Epoch 15/4000\n",
            "1684/1688 [============================>.] - ETA: 0s - loss: 0.2902 - accuracy: 0.9067\n",
            "Epoch 00015: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2903 - accuracy: 0.9067 - val_loss: 0.3273 - val_accuracy: 0.9037\n",
            "Epoch 16/4000\n",
            "1688/1688 [==============================] - ETA: 0s - loss: 0.2850 - accuracy: 0.9086\n",
            "Epoch 00016: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2850 - accuracy: 0.9086 - val_loss: 0.3182 - val_accuracy: 0.9118\n",
            "Epoch 17/4000\n",
            "1684/1688 [============================>.] - ETA: 0s - loss: 0.2819 - accuracy: 0.9108\n",
            "Epoch 00017: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2823 - accuracy: 0.9107 - val_loss: 0.3175 - val_accuracy: 0.9027\n",
            "Epoch 18/4000\n",
            "1674/1688 [============================>.] - ETA: 0s - loss: 0.2810 - accuracy: 0.9079\n",
            "Epoch 00018: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2806 - accuracy: 0.9079 - val_loss: 0.3597 - val_accuracy: 0.9078\n",
            "Epoch 19/4000\n",
            "1679/1688 [============================>.] - ETA: 0s - loss: 0.2836 - accuracy: 0.9113\n",
            "Epoch 00019: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2838 - accuracy: 0.9112 - val_loss: 0.3516 - val_accuracy: 0.9007\n",
            "Epoch 20/4000\n",
            "1680/1688 [============================>.] - ETA: 0s - loss: 0.2618 - accuracy: 0.9156\n",
            "Epoch 00020: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2619 - accuracy: 0.9157 - val_loss: 0.3123 - val_accuracy: 0.9143\n",
            "Epoch 21/4000\n",
            "1675/1688 [============================>.] - ETA: 0s - loss: 0.2761 - accuracy: 0.9126\n",
            "Epoch 00021: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2760 - accuracy: 0.9126 - val_loss: 0.3631 - val_accuracy: 0.8872\n",
            "Epoch 22/4000\n",
            "1682/1688 [============================>.] - ETA: 0s - loss: 0.2772 - accuracy: 0.9146\n",
            "Epoch 00022: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2777 - accuracy: 0.9146 - val_loss: 0.3315 - val_accuracy: 0.9153\n",
            "Epoch 23/4000\n",
            "1676/1688 [============================>.] - ETA: 0s - loss: 0.2557 - accuracy: 0.9179\n",
            "Epoch 00023: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2553 - accuracy: 0.9180 - val_loss: 0.3474 - val_accuracy: 0.9120\n",
            "Epoch 24/4000\n",
            "1675/1688 [============================>.] - ETA: 0s - loss: 0.2597 - accuracy: 0.9172\n",
            "Epoch 00024: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2599 - accuracy: 0.9173 - val_loss: 0.3334 - val_accuracy: 0.9133\n",
            "Epoch 25/4000\n",
            "1684/1688 [============================>.] - ETA: 0s - loss: 0.2663 - accuracy: 0.9187\n",
            "Epoch 00025: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2661 - accuracy: 0.9187 - val_loss: 0.3303 - val_accuracy: 0.9110\n",
            "Epoch 26/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.2696 - accuracy: 0.9169\n",
            "Epoch 00026: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2697 - accuracy: 0.9169 - val_loss: 0.3686 - val_accuracy: 0.9098\n",
            "Epoch 27/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.2680 - accuracy: 0.9187\n",
            "Epoch 00027: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2680 - accuracy: 0.9187 - val_loss: 0.3767 - val_accuracy: 0.9098\n",
            "Epoch 28/4000\n",
            "1679/1688 [============================>.] - ETA: 0s - loss: 0.2585 - accuracy: 0.9202\n",
            "Epoch 00028: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2585 - accuracy: 0.9202 - val_loss: 0.3810 - val_accuracy: 0.9182\n",
            "Epoch 29/4000\n",
            "1688/1688 [==============================] - ETA: 0s - loss: 0.2449 - accuracy: 0.9246\n",
            "Epoch 00029: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2449 - accuracy: 0.9246 - val_loss: 0.3866 - val_accuracy: 0.9008\n",
            "Epoch 30/4000\n",
            "1677/1688 [============================>.] - ETA: 0s - loss: 0.2564 - accuracy: 0.9208\n",
            "Epoch 00030: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2560 - accuracy: 0.9209 - val_loss: 0.4179 - val_accuracy: 0.9130\n",
            "Epoch 31/4000\n",
            "1688/1688 [==============================] - ETA: 0s - loss: 0.2508 - accuracy: 0.9231\n",
            "Epoch 00031: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2508 - accuracy: 0.9231 - val_loss: 0.3867 - val_accuracy: 0.9143\n",
            "Epoch 32/4000\n",
            "1688/1688 [==============================] - ETA: 0s - loss: 0.2499 - accuracy: 0.9233\n",
            "Epoch 00032: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2499 - accuracy: 0.9233 - val_loss: 0.3601 - val_accuracy: 0.9072\n",
            "Epoch 33/4000\n",
            "1683/1688 [============================>.] - ETA: 0s - loss: 0.2525 - accuracy: 0.9249\n",
            "Epoch 00033: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2522 - accuracy: 0.9249 - val_loss: 0.3990 - val_accuracy: 0.9008\n",
            "Epoch 34/4000\n",
            "1683/1688 [============================>.] - ETA: 0s - loss: 0.2453 - accuracy: 0.9263\n",
            "Epoch 00034: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2452 - accuracy: 0.9263 - val_loss: 0.3516 - val_accuracy: 0.9100\n",
            "Epoch 35/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2491 - accuracy: 0.9249\n",
            "Epoch 00035: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2491 - accuracy: 0.9249 - val_loss: 0.3987 - val_accuracy: 0.9152\n",
            "Epoch 36/4000\n",
            "1680/1688 [============================>.] - ETA: 0s - loss: 0.2381 - accuracy: 0.9278\n",
            "Epoch 00036: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2380 - accuracy: 0.9279 - val_loss: 0.3915 - val_accuracy: 0.9162\n",
            "Epoch 37/4000\n",
            "1679/1688 [============================>.] - ETA: 0s - loss: 0.2481 - accuracy: 0.9257\n",
            "Epoch 00037: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2481 - accuracy: 0.9258 - val_loss: 0.4101 - val_accuracy: 0.9135\n",
            "Epoch 38/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.2403 - accuracy: 0.9285\n",
            "Epoch 00038: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2401 - accuracy: 0.9285 - val_loss: 0.4241 - val_accuracy: 0.9130\n",
            "Epoch 39/4000\n",
            "1688/1688 [==============================] - ETA: 0s - loss: 0.2347 - accuracy: 0.9276\n",
            "Epoch 00039: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2347 - accuracy: 0.9276 - val_loss: 0.4960 - val_accuracy: 0.9128\n",
            "Epoch 40/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.2485 - accuracy: 0.9263\n",
            "Epoch 00040: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2484 - accuracy: 0.9262 - val_loss: 0.3875 - val_accuracy: 0.9173\n",
            "Epoch 41/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2340 - accuracy: 0.9303\n",
            "Epoch 00041: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2340 - accuracy: 0.9303 - val_loss: 0.4596 - val_accuracy: 0.9150\n",
            "Epoch 42/4000\n",
            "1675/1688 [============================>.] - ETA: 0s - loss: 0.2332 - accuracy: 0.9298\n",
            "Epoch 00042: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2327 - accuracy: 0.9299 - val_loss: 0.4501 - val_accuracy: 0.9195\n",
            "Epoch 43/4000\n",
            "1683/1688 [============================>.] - ETA: 0s - loss: 0.2390 - accuracy: 0.9280\n",
            "Epoch 00043: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2391 - accuracy: 0.9280 - val_loss: 0.4272 - val_accuracy: 0.9172\n",
            "Epoch 44/4000\n",
            "1684/1688 [============================>.] - ETA: 0s - loss: 0.2450 - accuracy: 0.9302\n",
            "Epoch 00044: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2446 - accuracy: 0.9303 - val_loss: 0.4348 - val_accuracy: 0.9142\n",
            "Epoch 45/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.2300 - accuracy: 0.9308\n",
            "Epoch 00045: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2301 - accuracy: 0.9309 - val_loss: 0.4796 - val_accuracy: 0.9135\n",
            "Epoch 46/4000\n",
            "1686/1688 [============================>.] - ETA: 0s - loss: 0.2352 - accuracy: 0.9297\n",
            "Epoch 00046: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2351 - accuracy: 0.9297 - val_loss: 0.4906 - val_accuracy: 0.9105\n",
            "Epoch 47/4000\n",
            "1682/1688 [============================>.] - ETA: 0s - loss: 0.2331 - accuracy: 0.9326\n",
            "Epoch 00047: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2332 - accuracy: 0.9325 - val_loss: 0.5513 - val_accuracy: 0.9123\n",
            "Epoch 48/4000\n",
            "1678/1688 [============================>.] - ETA: 0s - loss: 0.2306 - accuracy: 0.9323\n",
            "Epoch 00048: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2306 - accuracy: 0.9322 - val_loss: 0.4591 - val_accuracy: 0.9133\n",
            "Epoch 49/4000\n",
            "1677/1688 [============================>.] - ETA: 0s - loss: 0.2521 - accuracy: 0.9280\n",
            "Epoch 00049: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2515 - accuracy: 0.9282 - val_loss: 0.5228 - val_accuracy: 0.9177\n",
            "Epoch 50/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.2332 - accuracy: 0.9318\n",
            "Epoch 00050: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2339 - accuracy: 0.9317 - val_loss: 0.5471 - val_accuracy: 0.9077\n",
            "Epoch 51/4000\n",
            "1683/1688 [============================>.] - ETA: 0s - loss: 0.2352 - accuracy: 0.9304\n",
            "Epoch 00051: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2351 - accuracy: 0.9304 - val_loss: 0.5866 - val_accuracy: 0.9075\n",
            "Epoch 52/4000\n",
            "1677/1688 [============================>.] - ETA: 0s - loss: 0.2337 - accuracy: 0.9321\n",
            "Epoch 00052: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2340 - accuracy: 0.9320 - val_loss: 0.4879 - val_accuracy: 0.9112\n",
            "Epoch 53/4000\n",
            "1684/1688 [============================>.] - ETA: 0s - loss: 0.2397 - accuracy: 0.9313\n",
            "Epoch 00053: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2396 - accuracy: 0.9313 - val_loss: 0.6066 - val_accuracy: 0.9162\n",
            "Epoch 54/4000\n",
            "1686/1688 [============================>.] - ETA: 0s - loss: 0.2442 - accuracy: 0.9320\n",
            "Epoch 00054: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2443 - accuracy: 0.9320 - val_loss: 0.5215 - val_accuracy: 0.9155\n",
            "Epoch 55/4000\n",
            "1679/1688 [============================>.] - ETA: 0s - loss: 0.2503 - accuracy: 0.9307\n",
            "Epoch 00055: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2508 - accuracy: 0.9306 - val_loss: 0.4883 - val_accuracy: 0.9058\n",
            "Epoch 56/4000\n",
            "1680/1688 [============================>.] - ETA: 0s - loss: 0.2417 - accuracy: 0.9323\n",
            "Epoch 00056: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2424 - accuracy: 0.9322 - val_loss: 0.5707 - val_accuracy: 0.9152\n",
            "Epoch 57/4000\n",
            "1683/1688 [============================>.] - ETA: 0s - loss: 0.2361 - accuracy: 0.9314\n",
            "Epoch 00057: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2362 - accuracy: 0.9313 - val_loss: 0.5092 - val_accuracy: 0.9075\n",
            "Epoch 58/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2187 - accuracy: 0.9358\n",
            "Epoch 00058: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2187 - accuracy: 0.9358 - val_loss: 0.5306 - val_accuracy: 0.9090\n",
            "Epoch 59/4000\n",
            "1679/1688 [============================>.] - ETA: 0s - loss: 0.2357 - accuracy: 0.9336\n",
            "Epoch 00059: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2371 - accuracy: 0.9335 - val_loss: 0.4823 - val_accuracy: 0.9132\n",
            "Epoch 60/4000\n",
            "1684/1688 [============================>.] - ETA: 0s - loss: 0.2224 - accuracy: 0.9358\n",
            "Epoch 00060: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2222 - accuracy: 0.9358 - val_loss: 0.5886 - val_accuracy: 0.9138\n",
            "Epoch 61/4000\n",
            "1686/1688 [============================>.] - ETA: 0s - loss: 0.2266 - accuracy: 0.9352\n",
            "Epoch 00061: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2266 - accuracy: 0.9352 - val_loss: 0.5575 - val_accuracy: 0.9175\n",
            "Epoch 62/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.2537 - accuracy: 0.9312\n",
            "Epoch 00062: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2537 - accuracy: 0.9311 - val_loss: 0.5393 - val_accuracy: 0.9018\n",
            "Epoch 63/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2276 - accuracy: 0.9341\n",
            "Epoch 00063: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2276 - accuracy: 0.9341 - val_loss: 0.5574 - val_accuracy: 0.9060\n",
            "Epoch 64/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.2273 - accuracy: 0.9346\n",
            "Epoch 00064: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2271 - accuracy: 0.9346 - val_loss: 0.5979 - val_accuracy: 0.9092\n",
            "Epoch 65/4000\n",
            "1682/1688 [============================>.] - ETA: 0s - loss: 0.2399 - accuracy: 0.9322\n",
            "Epoch 00065: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2399 - accuracy: 0.9322 - val_loss: 0.4892 - val_accuracy: 0.9053\n",
            "Epoch 66/4000\n",
            "1675/1688 [============================>.] - ETA: 0s - loss: 0.2259 - accuracy: 0.9356\n",
            "Epoch 00066: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2254 - accuracy: 0.9357 - val_loss: 0.6022 - val_accuracy: 0.9185\n",
            "Epoch 67/4000\n",
            "1681/1688 [============================>.] - ETA: 0s - loss: 0.2273 - accuracy: 0.9349\n",
            "Epoch 00067: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2272 - accuracy: 0.9350 - val_loss: 0.6104 - val_accuracy: 0.9137\n",
            "Epoch 68/4000\n",
            "1680/1688 [============================>.] - ETA: 0s - loss: 0.2155 - accuracy: 0.9371\n",
            "Epoch 00068: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2157 - accuracy: 0.9370 - val_loss: 0.5875 - val_accuracy: 0.9142\n",
            "Epoch 69/4000\n",
            "1676/1688 [============================>.] - ETA: 0s - loss: 0.2380 - accuracy: 0.9323\n",
            "Epoch 00069: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2379 - accuracy: 0.9321 - val_loss: 0.5735 - val_accuracy: 0.9153\n",
            "Epoch 70/4000\n",
            "1679/1688 [============================>.] - ETA: 0s - loss: 0.2181 - accuracy: 0.9376\n",
            "Epoch 00070: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2182 - accuracy: 0.9375 - val_loss: 0.5836 - val_accuracy: 0.9125\n",
            "Epoch 71/4000\n",
            "1679/1688 [============================>.] - ETA: 0s - loss: 0.2405 - accuracy: 0.9339\n",
            "Epoch 00071: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2400 - accuracy: 0.9340 - val_loss: 0.6829 - val_accuracy: 0.9135\n",
            "Epoch 72/4000\n",
            "1688/1688 [==============================] - ETA: 0s - loss: 0.2400 - accuracy: 0.9344\n",
            "Epoch 00072: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2400 - accuracy: 0.9344 - val_loss: 0.5295 - val_accuracy: 0.9037\n",
            "Epoch 73/4000\n",
            "1679/1688 [============================>.] - ETA: 0s - loss: 0.2311 - accuracy: 0.9344\n",
            "Epoch 00073: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2311 - accuracy: 0.9343 - val_loss: 0.5413 - val_accuracy: 0.9123\n",
            "Epoch 74/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2133 - accuracy: 0.9385\n",
            "Epoch 00074: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2132 - accuracy: 0.9385 - val_loss: 0.5875 - val_accuracy: 0.9143\n",
            "Epoch 75/4000\n",
            "1682/1688 [============================>.] - ETA: 0s - loss: 0.2367 - accuracy: 0.9347\n",
            "Epoch 00075: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2364 - accuracy: 0.9347 - val_loss: 0.6926 - val_accuracy: 0.9167\n",
            "Epoch 76/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2313 - accuracy: 0.9375\n",
            "Epoch 00076: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2314 - accuracy: 0.9375 - val_loss: 0.7113 - val_accuracy: 0.9122\n",
            "Epoch 77/4000\n",
            "1683/1688 [============================>.] - ETA: 0s - loss: 0.2297 - accuracy: 0.9359\n",
            "Epoch 00077: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2299 - accuracy: 0.9360 - val_loss: 0.8140 - val_accuracy: 0.9163\n",
            "Epoch 78/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.2405 - accuracy: 0.9358\n",
            "Epoch 00078: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2402 - accuracy: 0.9359 - val_loss: 0.6596 - val_accuracy: 0.9118\n",
            "Epoch 79/4000\n",
            "1684/1688 [============================>.] - ETA: 0s - loss: 0.1993 - accuracy: 0.9432\n",
            "Epoch 00079: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.1990 - accuracy: 0.9433 - val_loss: 0.6827 - val_accuracy: 0.9137\n",
            "Epoch 80/4000\n",
            "1677/1688 [============================>.] - ETA: 0s - loss: 0.2324 - accuracy: 0.9358\n",
            "Epoch 00080: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2323 - accuracy: 0.9358 - val_loss: 0.7589 - val_accuracy: 0.9125\n",
            "Epoch 81/4000\n",
            "1677/1688 [============================>.] - ETA: 0s - loss: 0.2402 - accuracy: 0.9327\n",
            "Epoch 00081: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2414 - accuracy: 0.9327 - val_loss: 0.6908 - val_accuracy: 0.9053\n",
            "Epoch 82/4000\n",
            "1688/1688 [==============================] - ETA: 0s - loss: 0.2436 - accuracy: 0.9328\n",
            "Epoch 00082: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2436 - accuracy: 0.9328 - val_loss: 0.6216 - val_accuracy: 0.9132\n",
            "Epoch 83/4000\n",
            "1674/1688 [============================>.] - ETA: 0s - loss: 0.2100 - accuracy: 0.9412\n",
            "Epoch 00083: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2100 - accuracy: 0.9411 - val_loss: 0.8285 - val_accuracy: 0.9152\n",
            "Epoch 84/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2361 - accuracy: 0.9343\n",
            "Epoch 00084: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2361 - accuracy: 0.9343 - val_loss: 0.8326 - val_accuracy: 0.9115\n",
            "Epoch 85/4000\n",
            "1688/1688 [==============================] - ETA: 0s - loss: 0.2162 - accuracy: 0.9401\n",
            "Epoch 00085: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2162 - accuracy: 0.9401 - val_loss: 0.9396 - val_accuracy: 0.9140\n",
            "Epoch 86/4000\n",
            "1681/1688 [============================>.] - ETA: 0s - loss: 0.2344 - accuracy: 0.9381\n",
            "Epoch 00086: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2340 - accuracy: 0.9381 - val_loss: 0.7544 - val_accuracy: 0.9140\n",
            "Epoch 87/4000\n",
            "1688/1688 [==============================] - ETA: 0s - loss: 0.2415 - accuracy: 0.9358\n",
            "Epoch 00087: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2415 - accuracy: 0.9358 - val_loss: 0.7276 - val_accuracy: 0.9087\n",
            "Epoch 88/4000\n",
            "1677/1688 [============================>.] - ETA: 0s - loss: 0.2512 - accuracy: 0.9318\n",
            "Epoch 00088: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2509 - accuracy: 0.9317 - val_loss: 0.8789 - val_accuracy: 0.9117\n",
            "Epoch 89/4000\n",
            "1680/1688 [============================>.] - ETA: 0s - loss: 0.2271 - accuracy: 0.9378\n",
            "Epoch 00089: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2274 - accuracy: 0.9376 - val_loss: 0.8962 - val_accuracy: 0.9082\n",
            "Epoch 90/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.2489 - accuracy: 0.9334\n",
            "Epoch 00090: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2487 - accuracy: 0.9334 - val_loss: 0.7938 - val_accuracy: 0.9092\n",
            "Epoch 91/4000\n",
            "1676/1688 [============================>.] - ETA: 0s - loss: 0.2300 - accuracy: 0.9389\n",
            "Epoch 00091: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2305 - accuracy: 0.9388 - val_loss: 0.7597 - val_accuracy: 0.9102\n",
            "Epoch 92/4000\n",
            "1677/1688 [============================>.] - ETA: 0s - loss: 0.2267 - accuracy: 0.9399\n",
            "Epoch 00092: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2285 - accuracy: 0.9399 - val_loss: 0.6838 - val_accuracy: 0.9132\n",
            "Epoch 93/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2567 - accuracy: 0.9305\n",
            "Epoch 00093: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2567 - accuracy: 0.9305 - val_loss: 0.8022 - val_accuracy: 0.9145\n",
            "Epoch 94/4000\n",
            "1680/1688 [============================>.] - ETA: 0s - loss: 0.2383 - accuracy: 0.9366\n",
            "Epoch 00094: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2391 - accuracy: 0.9366 - val_loss: 0.6454 - val_accuracy: 0.9070\n",
            "Epoch 95/4000\n",
            "1684/1688 [============================>.] - ETA: 0s - loss: 0.2425 - accuracy: 0.9334\n",
            "Epoch 00095: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2424 - accuracy: 0.9334 - val_loss: 0.7916 - val_accuracy: 0.9130\n",
            "Epoch 96/4000\n",
            "1674/1688 [============================>.] - ETA: 0s - loss: 0.2403 - accuracy: 0.9326\n",
            "Epoch 00096: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2411 - accuracy: 0.9325 - val_loss: 0.6653 - val_accuracy: 0.9135\n",
            "Epoch 97/4000\n",
            "1682/1688 [============================>.] - ETA: 0s - loss: 0.2374 - accuracy: 0.9372\n",
            "Epoch 00097: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2373 - accuracy: 0.9372 - val_loss: 0.6794 - val_accuracy: 0.9108\n",
            "Epoch 98/4000\n",
            "1682/1688 [============================>.] - ETA: 0s - loss: 0.2267 - accuracy: 0.9366\n",
            "Epoch 00098: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2266 - accuracy: 0.9366 - val_loss: 0.7005 - val_accuracy: 0.9018\n",
            "Epoch 99/4000\n",
            "1684/1688 [============================>.] - ETA: 0s - loss: 0.2469 - accuracy: 0.9338\n",
            "Epoch 00099: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2468 - accuracy: 0.9338 - val_loss: 0.5934 - val_accuracy: 0.9135\n",
            "Epoch 100/4000\n",
            "1681/1688 [============================>.] - ETA: 0s - loss: 0.2725 - accuracy: 0.9330\n",
            "Epoch 00100: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2722 - accuracy: 0.9331 - val_loss: 0.8629 - val_accuracy: 0.9163\n",
            "Epoch 101/4000\n",
            "1683/1688 [============================>.] - ETA: 0s - loss: 0.2383 - accuracy: 0.9345\n",
            "Epoch 00101: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2382 - accuracy: 0.9345 - val_loss: 0.8504 - val_accuracy: 0.9142\n",
            "Epoch 102/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2492 - accuracy: 0.9322\n",
            "Epoch 00102: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2491 - accuracy: 0.9322 - val_loss: 0.6684 - val_accuracy: 0.8987\n",
            "Epoch 103/4000\n",
            "1679/1688 [============================>.] - ETA: 0s - loss: 0.2452 - accuracy: 0.9359\n",
            "Epoch 00103: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2448 - accuracy: 0.9360 - val_loss: 0.7763 - val_accuracy: 0.9130\n",
            "Epoch 104/4000\n",
            "1681/1688 [============================>.] - ETA: 0s - loss: 0.2276 - accuracy: 0.9373\n",
            "Epoch 00104: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2280 - accuracy: 0.9372 - val_loss: 0.7601 - val_accuracy: 0.9178\n",
            "Epoch 105/4000\n",
            "1683/1688 [============================>.] - ETA: 0s - loss: 0.2393 - accuracy: 0.9340\n",
            "Epoch 00105: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2391 - accuracy: 0.9340 - val_loss: 0.5814 - val_accuracy: 0.9087\n",
            "Epoch 106/4000\n",
            "1678/1688 [============================>.] - ETA: 0s - loss: 0.2440 - accuracy: 0.9368\n",
            "Epoch 00106: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2442 - accuracy: 0.9366 - val_loss: 0.5852 - val_accuracy: 0.9132\n",
            "Epoch 107/4000\n",
            "1686/1688 [============================>.] - ETA: 0s - loss: 0.2819 - accuracy: 0.9297\n",
            "Epoch 00107: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2818 - accuracy: 0.9297 - val_loss: 0.5754 - val_accuracy: 0.9028\n",
            "Epoch 108/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2343 - accuracy: 0.9389\n",
            "Epoch 00108: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2342 - accuracy: 0.9389 - val_loss: 0.7982 - val_accuracy: 0.9167\n",
            "Epoch 109/4000\n",
            "1688/1688 [==============================] - ETA: 0s - loss: 0.2410 - accuracy: 0.9351\n",
            "Epoch 00109: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2410 - accuracy: 0.9351 - val_loss: 0.7088 - val_accuracy: 0.9143\n",
            "Epoch 110/4000\n",
            "1674/1688 [============================>.] - ETA: 0s - loss: 0.2470 - accuracy: 0.9343\n",
            "Epoch 00110: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2465 - accuracy: 0.9343 - val_loss: 0.9234 - val_accuracy: 0.9142\n",
            "Epoch 111/4000\n",
            "1682/1688 [============================>.] - ETA: 0s - loss: 0.2151 - accuracy: 0.9414\n",
            "Epoch 00111: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2156 - accuracy: 0.9414 - val_loss: 0.7431 - val_accuracy: 0.9133\n",
            "Epoch 112/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2636 - accuracy: 0.9334\n",
            "Epoch 00112: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2638 - accuracy: 0.9333 - val_loss: 0.7230 - val_accuracy: 0.9083\n",
            "Epoch 113/4000\n",
            "1680/1688 [============================>.] - ETA: 0s - loss: 0.2903 - accuracy: 0.9292\n",
            "Epoch 00113: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2897 - accuracy: 0.9293 - val_loss: 0.7770 - val_accuracy: 0.9085\n",
            "Epoch 114/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.2590 - accuracy: 0.9333\n",
            "Epoch 00114: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2592 - accuracy: 0.9332 - val_loss: 0.7197 - val_accuracy: 0.9023\n",
            "Epoch 115/4000\n",
            "1678/1688 [============================>.] - ETA: 0s - loss: 0.2935 - accuracy: 0.9263\n",
            "Epoch 00115: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2926 - accuracy: 0.9265 - val_loss: 0.7772 - val_accuracy: 0.9137\n",
            "Epoch 116/4000\n",
            "1676/1688 [============================>.] - ETA: 0s - loss: 0.2719 - accuracy: 0.9301\n",
            "Epoch 00116: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2719 - accuracy: 0.9299 - val_loss: 0.5822 - val_accuracy: 0.9162\n",
            "Epoch 117/4000\n",
            "1676/1688 [============================>.] - ETA: 0s - loss: 0.2193 - accuracy: 0.9407\n",
            "Epoch 00117: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2187 - accuracy: 0.9407 - val_loss: 0.7246 - val_accuracy: 0.9158\n",
            "Epoch 118/4000\n",
            "1680/1688 [============================>.] - ETA: 0s - loss: 0.2470 - accuracy: 0.9349\n",
            "Epoch 00118: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2475 - accuracy: 0.9347 - val_loss: 0.7652 - val_accuracy: 0.9113\n",
            "Epoch 119/4000\n",
            "1673/1688 [============================>.] - ETA: 0s - loss: 0.2895 - accuracy: 0.9285\n",
            "Epoch 00119: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2951 - accuracy: 0.9276 - val_loss: 0.6928 - val_accuracy: 0.8648\n",
            "Epoch 120/4000\n",
            "1676/1688 [============================>.] - ETA: 0s - loss: 0.2440 - accuracy: 0.9338\n",
            "Epoch 00120: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2432 - accuracy: 0.9340 - val_loss: 1.0023 - val_accuracy: 0.9188\n",
            "Epoch 121/4000\n",
            "1688/1688 [==============================] - ETA: 0s - loss: 0.2356 - accuracy: 0.9372\n",
            "Epoch 00121: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2356 - accuracy: 0.9372 - val_loss: 0.8724 - val_accuracy: 0.9145\n",
            "Epoch 122/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2858 - accuracy: 0.9258\n",
            "Epoch 00122: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2857 - accuracy: 0.9258 - val_loss: 0.9426 - val_accuracy: 0.9187\n",
            "Epoch 123/4000\n",
            "1686/1688 [============================>.] - ETA: 0s - loss: 0.2276 - accuracy: 0.9389\n",
            "Epoch 00123: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2276 - accuracy: 0.9389 - val_loss: 0.7099 - val_accuracy: 0.9067\n",
            "Epoch 124/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.3056 - accuracy: 0.9248\n",
            "Epoch 00124: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3056 - accuracy: 0.9248 - val_loss: 1.0136 - val_accuracy: 0.9193\n",
            "Epoch 125/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2407 - accuracy: 0.9374\n",
            "Epoch 00125: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2408 - accuracy: 0.9373 - val_loss: 0.5533 - val_accuracy: 0.9118\n",
            "Epoch 126/4000\n",
            "1679/1688 [============================>.] - ETA: 0s - loss: 0.2550 - accuracy: 0.9322\n",
            "Epoch 00126: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2548 - accuracy: 0.9322 - val_loss: 0.8537 - val_accuracy: 0.9097\n",
            "Epoch 127/4000\n",
            "1675/1688 [============================>.] - ETA: 0s - loss: 0.2547 - accuracy: 0.9343\n",
            "Epoch 00127: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2552 - accuracy: 0.9342 - val_loss: 0.7355 - val_accuracy: 0.9118\n",
            "Epoch 128/4000\n",
            "1682/1688 [============================>.] - ETA: 0s - loss: 0.2267 - accuracy: 0.9382\n",
            "Epoch 00128: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2264 - accuracy: 0.9383 - val_loss: 0.9194 - val_accuracy: 0.9125\n",
            "Epoch 129/4000\n",
            "1686/1688 [============================>.] - ETA: 0s - loss: 0.2582 - accuracy: 0.9305\n",
            "Epoch 00129: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2583 - accuracy: 0.9305 - val_loss: 0.6256 - val_accuracy: 0.9123\n",
            "Epoch 130/4000\n",
            "1679/1688 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.9277\n",
            "Epoch 00130: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2866 - accuracy: 0.9276 - val_loss: 0.8493 - val_accuracy: 0.9137\n",
            "Epoch 131/4000\n",
            "1684/1688 [============================>.] - ETA: 0s - loss: 0.2720 - accuracy: 0.9303\n",
            "Epoch 00131: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2719 - accuracy: 0.9303 - val_loss: 0.5531 - val_accuracy: 0.9035\n",
            "Epoch 132/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.2527 - accuracy: 0.9327\n",
            "Epoch 00132: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2526 - accuracy: 0.9327 - val_loss: 0.6668 - val_accuracy: 0.9037\n",
            "Epoch 133/4000\n",
            "1674/1688 [============================>.] - ETA: 0s - loss: 0.2476 - accuracy: 0.9331\n",
            "Epoch 00133: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2471 - accuracy: 0.9331 - val_loss: 0.8499 - val_accuracy: 0.9168\n",
            "Epoch 134/4000\n",
            "1674/1688 [============================>.] - ETA: 0s - loss: 0.2771 - accuracy: 0.9303\n",
            "Epoch 00134: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2774 - accuracy: 0.9303 - val_loss: 0.7109 - val_accuracy: 0.9067\n",
            "Epoch 135/4000\n",
            "1676/1688 [============================>.] - ETA: 0s - loss: 0.2521 - accuracy: 0.9343\n",
            "Epoch 00135: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2517 - accuracy: 0.9343 - val_loss: 0.9135 - val_accuracy: 0.9155\n",
            "Epoch 136/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2671 - accuracy: 0.9317\n",
            "Epoch 00136: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2670 - accuracy: 0.9317 - val_loss: 0.6999 - val_accuracy: 0.9187\n",
            "Epoch 137/4000\n",
            "1682/1688 [============================>.] - ETA: 0s - loss: 0.2604 - accuracy: 0.9324\n",
            "Epoch 00137: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2607 - accuracy: 0.9324 - val_loss: 0.6070 - val_accuracy: 0.9042\n",
            "Epoch 138/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2716 - accuracy: 0.9300\n",
            "Epoch 00138: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2716 - accuracy: 0.9300 - val_loss: 1.0796 - val_accuracy: 0.9097\n",
            "Epoch 139/4000\n",
            "1674/1688 [============================>.] - ETA: 0s - loss: 0.2474 - accuracy: 0.9346\n",
            "Epoch 00139: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2479 - accuracy: 0.9345 - val_loss: 0.9695 - val_accuracy: 0.9102\n",
            "Epoch 140/4000\n",
            "1674/1688 [============================>.] - ETA: 0s - loss: 0.2825 - accuracy: 0.9309\n",
            "Epoch 00140: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2814 - accuracy: 0.9310 - val_loss: 0.8000 - val_accuracy: 0.9160\n",
            "Epoch 141/4000\n",
            "1686/1688 [============================>.] - ETA: 0s - loss: 0.2728 - accuracy: 0.9319\n",
            "Epoch 00141: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2727 - accuracy: 0.9319 - val_loss: 0.5488 - val_accuracy: 0.8952\n",
            "Epoch 142/4000\n",
            "1679/1688 [============================>.] - ETA: 0s - loss: 0.2535 - accuracy: 0.9334\n",
            "Epoch 00142: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2531 - accuracy: 0.9335 - val_loss: 0.9116 - val_accuracy: 0.9155\n",
            "Epoch 143/4000\n",
            "1679/1688 [============================>.] - ETA: 0s - loss: 0.2397 - accuracy: 0.9359\n",
            "Epoch 00143: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2391 - accuracy: 0.9359 - val_loss: 1.0355 - val_accuracy: 0.9113\n",
            "Epoch 144/4000\n",
            "1676/1688 [============================>.] - ETA: 0s - loss: 0.2944 - accuracy: 0.9258\n",
            "Epoch 00144: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2938 - accuracy: 0.9259 - val_loss: 0.7298 - val_accuracy: 0.9115\n",
            "Epoch 145/4000\n",
            "1684/1688 [============================>.] - ETA: 0s - loss: 0.2784 - accuracy: 0.9291\n",
            "Epoch 00145: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2786 - accuracy: 0.9291 - val_loss: 0.7692 - val_accuracy: 0.9115\n",
            "Epoch 146/4000\n",
            "1675/1688 [============================>.] - ETA: 0s - loss: 0.2599 - accuracy: 0.9310\n",
            "Epoch 00146: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2604 - accuracy: 0.9309 - val_loss: 0.8707 - val_accuracy: 0.9165\n",
            "Epoch 147/4000\n",
            "1686/1688 [============================>.] - ETA: 0s - loss: 0.2661 - accuracy: 0.9311\n",
            "Epoch 00147: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2662 - accuracy: 0.9311 - val_loss: 0.9989 - val_accuracy: 0.9102\n",
            "Epoch 148/4000\n",
            "1688/1688 [==============================] - ETA: 0s - loss: 0.2752 - accuracy: 0.9283\n",
            "Epoch 00148: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2752 - accuracy: 0.9283 - val_loss: 1.6173 - val_accuracy: 0.9123\n",
            "Epoch 149/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.2937 - accuracy: 0.9272\n",
            "Epoch 00149: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2937 - accuracy: 0.9272 - val_loss: 0.7323 - val_accuracy: 0.9068\n",
            "Epoch 150/4000\n",
            "1674/1688 [============================>.] - ETA: 0s - loss: 0.2709 - accuracy: 0.9282\n",
            "Epoch 00150: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2722 - accuracy: 0.9282 - val_loss: 0.7370 - val_accuracy: 0.9140\n",
            "Epoch 151/4000\n",
            "1681/1688 [============================>.] - ETA: 0s - loss: 0.2492 - accuracy: 0.9320\n",
            "Epoch 00151: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2492 - accuracy: 0.9321 - val_loss: 0.7563 - val_accuracy: 0.9125\n",
            "Epoch 152/4000\n",
            "1675/1688 [============================>.] - ETA: 0s - loss: 0.2777 - accuracy: 0.9279\n",
            "Epoch 00152: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2774 - accuracy: 0.9279 - val_loss: 0.8395 - val_accuracy: 0.9108\n",
            "Epoch 153/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2493 - accuracy: 0.9332\n",
            "Epoch 00153: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2495 - accuracy: 0.9332 - val_loss: 0.8496 - val_accuracy: 0.9092\n",
            "Epoch 154/4000\n",
            "1688/1688 [==============================] - ETA: 0s - loss: 0.2982 - accuracy: 0.9266\n",
            "Epoch 00154: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2982 - accuracy: 0.9266 - val_loss: 0.9135 - val_accuracy: 0.9082\n",
            "Epoch 155/4000\n",
            "1686/1688 [============================>.] - ETA: 0s - loss: 0.2982 - accuracy: 0.9248\n",
            "Epoch 00155: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2982 - accuracy: 0.9248 - val_loss: 0.7577 - val_accuracy: 0.9107\n",
            "Epoch 156/4000\n",
            "1688/1688 [==============================] - ETA: 0s - loss: 0.2577 - accuracy: 0.9324\n",
            "Epoch 00156: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2577 - accuracy: 0.9324 - val_loss: 1.0219 - val_accuracy: 0.9133\n",
            "Epoch 157/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.3364 - accuracy: 0.9175\n",
            "Epoch 00157: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3366 - accuracy: 0.9175 - val_loss: 0.6247 - val_accuracy: 0.9012\n",
            "Epoch 158/4000\n",
            "1677/1688 [============================>.] - ETA: 0s - loss: 0.2627 - accuracy: 0.9295\n",
            "Epoch 00158: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2623 - accuracy: 0.9296 - val_loss: 0.8988 - val_accuracy: 0.9148\n",
            "Epoch 159/4000\n",
            "1686/1688 [============================>.] - ETA: 0s - loss: 0.2996 - accuracy: 0.9254\n",
            "Epoch 00159: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2996 - accuracy: 0.9254 - val_loss: 0.9541 - val_accuracy: 0.9107\n",
            "Epoch 160/4000\n",
            "1680/1688 [============================>.] - ETA: 0s - loss: 0.2759 - accuracy: 0.9251\n",
            "Epoch 00160: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2753 - accuracy: 0.9253 - val_loss: 1.1129 - val_accuracy: 0.9162\n",
            "Epoch 161/4000\n",
            "1682/1688 [============================>.] - ETA: 0s - loss: 0.2761 - accuracy: 0.9288\n",
            "Epoch 00161: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2756 - accuracy: 0.9289 - val_loss: 0.9137 - val_accuracy: 0.9143\n",
            "Epoch 162/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.3029 - accuracy: 0.9209\n",
            "Epoch 00162: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3031 - accuracy: 0.9209 - val_loss: 0.7718 - val_accuracy: 0.9085\n",
            "Epoch 163/4000\n",
            "1686/1688 [============================>.] - ETA: 0s - loss: 0.2617 - accuracy: 0.9303\n",
            "Epoch 00163: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2633 - accuracy: 0.9303 - val_loss: 0.6701 - val_accuracy: 0.9057\n",
            "Epoch 164/4000\n",
            "1680/1688 [============================>.] - ETA: 0s - loss: 0.2505 - accuracy: 0.9298\n",
            "Epoch 00164: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2508 - accuracy: 0.9296 - val_loss: 1.1442 - val_accuracy: 0.9115\n",
            "Epoch 165/4000\n",
            "1677/1688 [============================>.] - ETA: 0s - loss: 0.2597 - accuracy: 0.9322\n",
            "Epoch 00165: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2593 - accuracy: 0.9322 - val_loss: 0.8620 - val_accuracy: 0.9152\n",
            "Epoch 166/4000\n",
            "1684/1688 [============================>.] - ETA: 0s - loss: 0.3368 - accuracy: 0.9209\n",
            "Epoch 00166: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.3369 - accuracy: 0.9209 - val_loss: 0.8989 - val_accuracy: 0.9133\n",
            "Epoch 167/4000\n",
            "1682/1688 [============================>.] - ETA: 0s - loss: 0.3114 - accuracy: 0.9218\n",
            "Epoch 00167: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3112 - accuracy: 0.9217 - val_loss: 0.8435 - val_accuracy: 0.9010\n",
            "Epoch 168/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2640 - accuracy: 0.9303\n",
            "Epoch 00168: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2639 - accuracy: 0.9303 - val_loss: 0.9770 - val_accuracy: 0.9055\n",
            "Epoch 169/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.3382 - accuracy: 0.9170\n",
            "Epoch 00169: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3380 - accuracy: 0.9170 - val_loss: 0.7821 - val_accuracy: 0.9063\n",
            "Epoch 170/4000\n",
            "1676/1688 [============================>.] - ETA: 0s - loss: 0.2612 - accuracy: 0.9325\n",
            "Epoch 00170: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2643 - accuracy: 0.9323 - val_loss: 0.9869 - val_accuracy: 0.9072\n",
            "Epoch 171/4000\n",
            "1678/1688 [============================>.] - ETA: 0s - loss: 0.2905 - accuracy: 0.9238\n",
            "Epoch 00171: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2908 - accuracy: 0.9237 - val_loss: 1.1032 - val_accuracy: 0.9133\n",
            "Epoch 172/4000\n",
            "1680/1688 [============================>.] - ETA: 0s - loss: 0.2892 - accuracy: 0.9255\n",
            "Epoch 00172: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2889 - accuracy: 0.9255 - val_loss: 0.8528 - val_accuracy: 0.9043\n",
            "Epoch 173/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2677 - accuracy: 0.9279\n",
            "Epoch 00173: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2677 - accuracy: 0.9279 - val_loss: 1.6675 - val_accuracy: 0.9057\n",
            "Epoch 174/4000\n",
            "1681/1688 [============================>.] - ETA: 0s - loss: 0.2925 - accuracy: 0.9231\n",
            "Epoch 00174: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2922 - accuracy: 0.9231 - val_loss: 0.6500 - val_accuracy: 0.9112\n",
            "Epoch 175/4000\n",
            "1686/1688 [============================>.] - ETA: 0s - loss: 0.3694 - accuracy: 0.9112\n",
            "Epoch 00175: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.3692 - accuracy: 0.9112 - val_loss: 0.6501 - val_accuracy: 0.9060\n",
            "Epoch 176/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.2700 - accuracy: 0.9285\n",
            "Epoch 00176: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2698 - accuracy: 0.9286 - val_loss: 0.9085 - val_accuracy: 0.9063\n",
            "Epoch 177/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2700 - accuracy: 0.9303\n",
            "Epoch 00177: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2700 - accuracy: 0.9303 - val_loss: 0.6567 - val_accuracy: 0.9112\n",
            "Epoch 178/4000\n",
            "1686/1688 [============================>.] - ETA: 0s - loss: 0.3214 - accuracy: 0.9200\n",
            "Epoch 00178: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3212 - accuracy: 0.9200 - val_loss: 1.0210 - val_accuracy: 0.9073\n",
            "Epoch 179/4000\n",
            "1681/1688 [============================>.] - ETA: 0s - loss: 0.2685 - accuracy: 0.9257\n",
            "Epoch 00179: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2685 - accuracy: 0.9256 - val_loss: 0.7802 - val_accuracy: 0.9095\n",
            "Epoch 180/4000\n",
            "1677/1688 [============================>.] - ETA: 0s - loss: 0.3196 - accuracy: 0.9196\n",
            "Epoch 00180: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3201 - accuracy: 0.9195 - val_loss: 0.6436 - val_accuracy: 0.9073\n",
            "Epoch 181/4000\n",
            "1684/1688 [============================>.] - ETA: 0s - loss: 0.2807 - accuracy: 0.9249\n",
            "Epoch 00181: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2811 - accuracy: 0.9248 - val_loss: 0.5221 - val_accuracy: 0.8987\n",
            "Epoch 182/4000\n",
            "1676/1688 [============================>.] - ETA: 0s - loss: 0.3115 - accuracy: 0.9177\n",
            "Epoch 00182: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3121 - accuracy: 0.9176 - val_loss: 0.6734 - val_accuracy: 0.9025\n",
            "Epoch 183/4000\n",
            "1675/1688 [============================>.] - ETA: 0s - loss: 0.2921 - accuracy: 0.9215\n",
            "Epoch 00183: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2915 - accuracy: 0.9216 - val_loss: 1.0503 - val_accuracy: 0.9063\n",
            "Epoch 184/4000\n",
            "1676/1688 [============================>.] - ETA: 0s - loss: 0.3175 - accuracy: 0.9196\n",
            "Epoch 00184: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3176 - accuracy: 0.9196 - val_loss: 0.8557 - val_accuracy: 0.9135\n",
            "Epoch 185/4000\n",
            "1677/1688 [============================>.] - ETA: 0s - loss: 0.2766 - accuracy: 0.9243\n",
            "Epoch 00185: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2762 - accuracy: 0.9244 - val_loss: 0.8323 - val_accuracy: 0.9120\n",
            "Epoch 186/4000\n",
            "1682/1688 [============================>.] - ETA: 0s - loss: 0.3089 - accuracy: 0.9238\n",
            "Epoch 00186: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3092 - accuracy: 0.9237 - val_loss: 1.0057 - val_accuracy: 0.9112\n",
            "Epoch 187/4000\n",
            "1675/1688 [============================>.] - ETA: 0s - loss: 0.3307 - accuracy: 0.9155\n",
            "Epoch 00187: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3310 - accuracy: 0.9153 - val_loss: 0.8723 - val_accuracy: 0.9048\n",
            "Epoch 188/4000\n",
            "1677/1688 [============================>.] - ETA: 0s - loss: 0.3240 - accuracy: 0.9156\n",
            "Epoch 00188: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3236 - accuracy: 0.9157 - val_loss: 0.7200 - val_accuracy: 0.9127\n",
            "Epoch 189/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.2337 - accuracy: 0.9373\n",
            "Epoch 00189: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2336 - accuracy: 0.9373 - val_loss: 0.9766 - val_accuracy: 0.9082\n",
            "Epoch 190/4000\n",
            "1678/1688 [============================>.] - ETA: 0s - loss: 0.2557 - accuracy: 0.9284\n",
            "Epoch 00190: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.2550 - accuracy: 0.9285 - val_loss: 1.0887 - val_accuracy: 0.9145\n",
            "Epoch 191/4000\n",
            "1687/1688 [============================>.] - ETA: 0s - loss: 0.3700 - accuracy: 0.9105\n",
            "Epoch 00191: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.3699 - accuracy: 0.9105 - val_loss: 1.0899 - val_accuracy: 0.9085\n",
            "Epoch 192/4000\n",
            "1683/1688 [============================>.] - ETA: 0s - loss: 0.3272 - accuracy: 0.9157\n",
            "Epoch 00192: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3277 - accuracy: 0.9156 - val_loss: 0.6374 - val_accuracy: 0.9062\n",
            "Epoch 193/4000\n",
            "1677/1688 [============================>.] - ETA: 0s - loss: 0.2983 - accuracy: 0.9213\n",
            "Epoch 00193: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2985 - accuracy: 0.9213 - val_loss: 0.7947 - val_accuracy: 0.9123\n",
            "Epoch 194/4000\n",
            "1682/1688 [============================>.] - ETA: 0s - loss: 0.3309 - accuracy: 0.9162\n",
            "Epoch 00194: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.3314 - accuracy: 0.9160 - val_loss: 0.6500 - val_accuracy: 0.8988\n",
            "Epoch 195/4000\n",
            "1680/1688 [============================>.] - ETA: 0s - loss: 0.3696 - accuracy: 0.9096\n",
            "Epoch 00195: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.3699 - accuracy: 0.9097 - val_loss: 0.7296 - val_accuracy: 0.9017\n",
            "Epoch 196/4000\n",
            "1677/1688 [============================>.] - ETA: 0s - loss: 0.3193 - accuracy: 0.9158\n",
            "Epoch 00196: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.3185 - accuracy: 0.9160 - val_loss: 1.0311 - val_accuracy: 0.9097\n",
            "Epoch 197/4000\n",
            "1677/1688 [============================>.] - ETA: 0s - loss: 0.3399 - accuracy: 0.9157\n",
            "Epoch 00197: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3393 - accuracy: 0.9159 - val_loss: 0.7646 - val_accuracy: 0.9115\n",
            "Epoch 198/4000\n",
            "1677/1688 [============================>.] - ETA: 0s - loss: 0.3142 - accuracy: 0.9177\n",
            "Epoch 00198: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3139 - accuracy: 0.9178 - val_loss: 0.8614 - val_accuracy: 0.9113\n",
            "Epoch 199/4000\n",
            "1681/1688 [============================>.] - ETA: 0s - loss: 0.2850 - accuracy: 0.9244\n",
            "Epoch 00199: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.2854 - accuracy: 0.9242 - val_loss: 0.8727 - val_accuracy: 0.9128\n",
            "Epoch 200/4000\n",
            "1674/1688 [============================>.] - ETA: 0s - loss: 0.3326 - accuracy: 0.9216\n",
            "Epoch 00200: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3335 - accuracy: 0.9214 - val_loss: 0.7661 - val_accuracy: 0.8983\n",
            "Epoch 201/4000\n",
            "1683/1688 [============================>.] - ETA: 0s - loss: 0.3129 - accuracy: 0.9183\n",
            "Epoch 00201: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3130 - accuracy: 0.9182 - val_loss: 0.4965 - val_accuracy: 0.8965\n",
            "Epoch 202/4000\n",
            "1678/1688 [============================>.] - ETA: 0s - loss: 0.3156 - accuracy: 0.9147\n",
            "Epoch 00202: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3160 - accuracy: 0.9146 - val_loss: 0.7523 - val_accuracy: 0.9037\n",
            "Epoch 203/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.3188 - accuracy: 0.9158\n",
            "Epoch 00203: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3185 - accuracy: 0.9158 - val_loss: 0.7063 - val_accuracy: 0.8940\n",
            "Epoch 204/4000\n",
            "1686/1688 [============================>.] - ETA: 0s - loss: 0.3344 - accuracy: 0.9165\n",
            "Epoch 00204: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 6s 4ms/step - loss: 0.3379 - accuracy: 0.9164 - val_loss: 0.8769 - val_accuracy: 0.9070\n",
            "Epoch 205/4000\n",
            "1679/1688 [============================>.] - ETA: 0s - loss: 0.3576 - accuracy: 0.9051\n",
            "Epoch 00205: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.3574 - accuracy: 0.9050 - val_loss: 0.6787 - val_accuracy: 0.9037\n",
            "Epoch 206/4000\n",
            "1685/1688 [============================>.] - ETA: 0s - loss: 0.3836 - accuracy: 0.9062\n",
            "Epoch 00206: saving model to /content/best_model.h5\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.3836 - accuracy: 0.9061 - val_loss: 1.0349 - val_accuracy: 0.9097\n",
            "Epoch 00206: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "imTbE_Fu8zxH",
        "outputId": "b75ebbc8-02b8-473a-9098-84232d86f346"
      },
      "source": [
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig=go.Figure()\n",
        "\n",
        "x=np.linspace(4, 100,endpoint=True)\n",
        "\n",
        "fig.add_trace(go.Scatter(x=x, y=model_history.history['accuracy'],\n",
        "                    mode='lines',\n",
        "                    name='Training accuracy'))\n",
        "fig.add_trace(go.Scatter(x=x, y=model_history.history['val_accuracy'],\n",
        "                    mode='lines',\n",
        "                    name='Validation accuracy'))\n",
        "fig.add_trace(go.Scatter(x=x, y=model_history.history['loss'],\n",
        "                    mode='lines',\n",
        "                    name='Training loss'))\n",
        "fig.add_trace(go.Scatter(x=x, y=model_history.history['val_loss'],\n",
        "                    mode='lines',\n",
        "                    name='Validation loss'))\n",
        "\n",
        "fig.show()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"17b4b25e-99f6-4195-baf8-32be9a42e25b\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"17b4b25e-99f6-4195-baf8-32be9a42e25b\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '17b4b25e-99f6-4195-baf8-32be9a42e25b',\n",
              "                        [{\"mode\": \"lines\", \"name\": \"Training accuracy\", \"type\": \"scatter\", \"x\": [4.0, 5.959183673469388, 7.918367346938775, 9.877551020408163, 11.83673469387755, 13.795918367346939, 15.755102040816325, 17.714285714285715, 19.6734693877551, 21.632653061224488, 23.591836734693878, 25.551020408163264, 27.51020408163265, 29.46938775510204, 31.428571428571427, 33.38775510204081, 35.3469387755102, 37.30612244897959, 39.265306122448976, 41.224489795918366, 43.183673469387756, 45.14285714285714, 47.10204081632653, 49.06122448979592, 51.0204081632653, 52.97959183673469, 54.93877551020408, 56.89795918367347, 58.857142857142854, 60.816326530612244, 62.775510204081634, 64.73469387755102, 66.6938775510204, 68.65306122448979, 70.61224489795919, 72.57142857142857, 74.53061224489795, 76.48979591836735, 78.44897959183673, 80.40816326530611, 82.36734693877551, 84.3265306122449, 86.28571428571428, 88.24489795918367, 90.20408163265306, 92.16326530612244, 94.12244897959184, 96.08163265306122, 98.0408163265306, 100.0], \"y\": [0.8785740733146667, 0.8855000138282776, 0.8892222046852112, 0.8923147916793823, 0.8936296105384827, 0.8956481218338013, 0.8979259133338928, 0.8981296420097351, 0.9011481404304504, 0.9041110873222351, 0.9060370326042175, 0.9067037105560303, 0.9085925817489624, 0.9106666445732117, 0.9079259037971497, 0.9112222194671631, 0.9157407283782959, 0.9126481413841248, 0.9145925641059875, 0.9180370569229126, 0.9173148274421692, 0.918666660785675, 0.9168888926506042, 0.9187036752700806, 0.9201666712760925, 0.9246481657028198, 0.9209073781967163, 0.9231296181678772, 0.9233148097991943, 0.924907386302948, 0.9263148307800293, 0.924907386302948, 0.9278888702392578, 0.9257962703704834, 0.9285370111465454, 0.9276296496391296, 0.9262222051620483, 0.9302963018417358, 0.9298518300056458, 0.9280370473861694, 0.9302963018417358, 0.9308518767356873, 0.9297037124633789, 0.9324629902839661, 0.9322222471237183, 0.9281666874885559, 0.9317222237586975, 0.9303703904151917, 0.9319999814033508, 0.9312962889671326, 0.932018518447876, 0.9305740594863892, 0.9322407245635986, 0.9312962889671326, 0.9358147978782654, 0.9334629774093628, 0.9358147978782654, 0.9351851940155029, 0.9311481714248657, 0.9341481328010559, 0.9346110820770264, 0.9322222471237183, 0.9356666803359985, 0.9349814653396606, 0.9370370507240295, 0.9321481585502625, 0.9374814629554749, 0.9339814782142639, 0.9343888759613037, 0.934333324432373, 0.9384629726409912, 0.9347222447395325, 0.9375, 0.935962975025177, 0.9358888864517212, 0.9432777762413025, 0.9357777833938599, 0.9327222108840942, 0.9327777624130249, 0.9411481618881226, 0.9342963099479675, 0.9400555491447449, 0.9381481409072876, 0.9357777833938599, 0.931685209274292, 0.9376296401023865, 0.9334444403648376, 0.9387778043746948, 0.9398703575134277, 0.9304629564285278, 0.9365741014480591, 0.9334259033203125, 0.9325185418128967, 0.937166690826416, 0.9366111159324646, 0.9337962865829468, 0.9330740571022034, 0.9344815015792847, 0.9322407245635986, 0.9359814524650574, 0.937240719795227, 0.9340000152587891, 0.9365925788879395, 0.9296851754188538, 0.9389444589614868, 0.9350740909576416, 0.934333324432373, 0.9413889050483704, 0.9333148002624512, 0.9293333292007446, 0.9332036972045898, 0.9264814853668213, 0.9299444556236267, 0.9407407641410828, 0.9346666932106018, 0.927574098110199, 0.9340184926986694, 0.9371851682662964, 0.9258333444595337, 0.938870370388031, 0.9247592687606812, 0.937333345413208, 0.9322222471237183, 0.9342222213745117, 0.9382963180541992, 0.930481493473053, 0.9275925755500793, 0.930314838886261, 0.9326851963996887, 0.933055579662323, 0.9303333163261414, 0.9343147873878479, 0.9317036867141724, 0.9323703646659851, 0.9300000071525574, 0.9345185160636902, 0.9310185313224792, 0.9318703413009644, 0.9334999918937683, 0.9359074234962463, 0.9259259104728699, 0.9290925860404968, 0.9308518767356873, 0.9310926198959351, 0.9283333420753479, 0.9272407293319702, 0.9281851649284363, 0.9320555329322815, 0.9278518557548523, 0.9331851601600647, 0.9266296029090881, 0.9247778058052063, 0.93235182762146, 0.9175184965133667, 0.9295740723609924, 0.9254074096679688, 0.9252777695655823, 0.9289259314537048, 0.9208703637123108, 0.930314838886261, 0.9296481609344482, 0.9322222471237183, 0.9208518266677856, 0.9216851592063904, 0.9302592873573303, 0.9169999957084656, 0.9322962760925293, 0.9236851930618286, 0.9255370497703552, 0.9278888702392578, 0.9230926036834717, 0.9112036824226379, 0.9285740852355957, 0.9303333163261414, 0.920037031173706, 0.925611138343811, 0.9194814562797546, 0.9248148202896118, 0.9176111221313477, 0.9216296076774597, 0.9196296334266663, 0.924407422542572, 0.9237222075462341, 0.915314793586731, 0.9156851768493652, 0.9372962713241577, 0.9284814596176147, 0.9104814529418945, 0.9155740737915039, 0.9213333129882812, 0.9159629344940186, 0.9096851944923401, 0.916018545627594, 0.9158518314361572, 0.9177963137626648, 0.92424076795578, 0.9214259386062622, 0.9181851744651794, 0.9146296381950378, 0.9158333539962769, 0.9164444208145142, 0.9049814939498901, 0.9061296582221985]}, {\"mode\": \"lines\", \"name\": \"Validation accuracy\", \"type\": \"scatter\", \"x\": [4.0, 5.959183673469388, 7.918367346938775, 9.877551020408163, 11.83673469387755, 13.795918367346939, 15.755102040816325, 17.714285714285715, 19.6734693877551, 21.632653061224488, 23.591836734693878, 25.551020408163264, 27.51020408163265, 29.46938775510204, 31.428571428571427, 33.38775510204081, 35.3469387755102, 37.30612244897959, 39.265306122448976, 41.224489795918366, 43.183673469387756, 45.14285714285714, 47.10204081632653, 49.06122448979592, 51.0204081632653, 52.97959183673469, 54.93877551020408, 56.89795918367347, 58.857142857142854, 60.816326530612244, 62.775510204081634, 64.73469387755102, 66.6938775510204, 68.65306122448979, 70.61224489795919, 72.57142857142857, 74.53061224489795, 76.48979591836735, 78.44897959183673, 80.40816326530611, 82.36734693877551, 84.3265306122449, 86.28571428571428, 88.24489795918367, 90.20408163265306, 92.16326530612244, 94.12244897959184, 96.08163265306122, 98.0408163265306, 100.0], \"y\": [0.8995000123977661, 0.9043333530426025, 0.9058333039283752, 0.9014999866485596, 0.9001666903495789, 0.9128333330154419, 0.9053333401679993, 0.9028333425521851, 0.902999997138977, 0.9096666574478149, 0.9066666960716248, 0.9036666750907898, 0.9118333458900452, 0.9026666879653931, 0.9078333377838135, 0.9006666541099548, 0.9143333435058594, 0.8871666789054871, 0.9153333306312561, 0.9120000004768372, 0.9133333563804626, 0.9110000133514404, 0.9098333120346069, 0.9098333120346069, 0.9181666374206543, 0.9008333086967468, 0.9129999876022339, 0.9143333435058594, 0.9071666598320007, 0.9008333086967468, 0.9100000262260437, 0.9151666760444641, 0.9161666631698608, 0.9135000109672546, 0.9129999876022339, 0.9128333330154419, 0.9173333048820496, 0.9150000214576721, 0.9194999933242798, 0.9171666502952576, 0.9141666889190674, 0.9135000109672546, 0.9104999899864197, 0.9123333096504211, 0.9133333563804626, 0.9176666736602783, 0.9076666831970215, 0.9075000286102295, 0.9111666679382324, 0.9161666631698608, 0.9154999852180481, 0.9058333039283752, 0.9151666760444641, 0.9075000286102295, 0.9089999794960022, 0.9131666421890259, 0.9138333201408386, 0.9175000190734863, 0.9018333554267883, 0.906000018119812, 0.909166693687439, 0.9053333401679993, 0.9185000061988831, 0.9136666655540466, 0.9141666889190674, 0.9153333306312561, 0.9125000238418579, 0.9135000109672546, 0.9036666750907898, 0.9123333096504211, 0.9143333435058594, 0.9166666865348816, 0.9121666550636292, 0.9163333177566528, 0.9118333458900452, 0.9136666655540466, 0.9125000238418579, 0.9053333401679993, 0.9131666421890259, 0.9151666760444641, 0.9114999771118164, 0.9139999747276306, 0.9139999747276306, 0.9086666703224182, 0.9116666913032532, 0.9081666469573975, 0.909166693687439, 0.9101666808128357, 0.9131666421890259, 0.9144999980926514, 0.9070000052452087, 0.9129999876022339, 0.9135000109672546, 0.9108333587646484, 0.9018333554267883, 0.9135000109672546, 0.9163333177566528, 0.9141666889190674, 0.8986666798591614, 0.9129999876022339, 0.9178333282470703, 0.9086666703224182, 0.9131666421890259, 0.9028333425521851, 0.9166666865348816, 0.9143333435058594, 0.9141666889190674, 0.9133333563804626, 0.9083333611488342, 0.9085000157356262, 0.9023333191871643, 0.9136666655540466, 0.9161666631698608, 0.9158333539962769, 0.9113333225250244, 0.8648333549499512, 0.918833315372467, 0.9144999980926514, 0.918666660785675, 0.9066666960716248, 0.9193333387374878, 0.9118333458900452, 0.9096666574478149, 0.9118333458900452, 0.9125000238418579, 0.9123333096504211, 0.9136666655540466, 0.9035000205039978, 0.9036666750907898, 0.9168333411216736, 0.9066666960716248, 0.9154999852180481, 0.918666660785675, 0.9041666388511658, 0.9096666574478149, 0.9101666808128357, 0.9160000085830688, 0.8951666951179504, 0.9154999852180481, 0.9113333225250244, 0.9114999771118164, 0.9114999771118164, 0.9164999723434448, 0.9101666808128357, 0.9123333096504211, 0.9068333506584167, 0.9139999747276306, 0.9125000238418579, 0.9108333587646484, 0.909166693687439, 0.9081666469573975, 0.9106666445732117, 0.9133333563804626, 0.9011666774749756, 0.9148333072662354, 0.9106666445732117, 0.9161666631698608, 0.9143333435058594, 0.9085000157356262, 0.9056666493415833, 0.9114999771118164, 0.9151666760444641, 0.9133333563804626, 0.9010000228881836, 0.9054999947547913, 0.906333327293396, 0.9071666598320007, 0.9133333563804626, 0.9043333530426025, 0.9056666493415833, 0.9111666679382324, 0.906000018119812, 0.906333327293396, 0.9111666679382324, 0.9073333144187927, 0.909500002861023, 0.9073333144187927, 0.8986666798591614, 0.9024999737739563, 0.906333327293396, 0.9135000109672546, 0.9120000004768372, 0.9111666679382324, 0.9048333168029785, 0.9126666784286499, 0.9081666469573975, 0.9144999980926514, 0.9085000157356262, 0.906166672706604, 0.9123333096504211, 0.8988333344459534, 0.9016666412353516, 0.9096666574478149, 0.9114999771118164, 0.9113333225250244, 0.9128333330154419, 0.8983333110809326, 0.8964999914169312, 0.9036666750907898, 0.8939999938011169, 0.9070000052452087, 0.9036666750907898, 0.9096666574478149]}, {\"mode\": \"lines\", \"name\": \"Training loss\", \"type\": \"scatter\", \"x\": [4.0, 5.959183673469388, 7.918367346938775, 9.877551020408163, 11.83673469387755, 13.795918367346939, 15.755102040816325, 17.714285714285715, 19.6734693877551, 21.632653061224488, 23.591836734693878, 25.551020408163264, 27.51020408163265, 29.46938775510204, 31.428571428571427, 33.38775510204081, 35.3469387755102, 37.30612244897959, 39.265306122448976, 41.224489795918366, 43.183673469387756, 45.14285714285714, 47.10204081632653, 49.06122448979592, 51.0204081632653, 52.97959183673469, 54.93877551020408, 56.89795918367347, 58.857142857142854, 60.816326530612244, 62.775510204081634, 64.73469387755102, 66.6938775510204, 68.65306122448979, 70.61224489795919, 72.57142857142857, 74.53061224489795, 76.48979591836735, 78.44897959183673, 80.40816326530611, 82.36734693877551, 84.3265306122449, 86.28571428571428, 88.24489795918367, 90.20408163265306, 92.16326530612244, 94.12244897959184, 96.08163265306122, 98.0408163265306, 100.0], \"y\": [0.36048442125320435, 0.3412620425224304, 0.3343588411808014, 0.32550930976867676, 0.3240791857242584, 0.31704744696617126, 0.3110058903694153, 0.3084799647331238, 0.30590593814849854, 0.2932249903678894, 0.29219284653663635, 0.2903060019016266, 0.28501516580581665, 0.2823021709918976, 0.2805911600589752, 0.28384092450141907, 0.26187050342559814, 0.27604377269744873, 0.27769026160240173, 0.2553468942642212, 0.25993067026138306, 0.2660635709762573, 0.2696586847305298, 0.2679502069950104, 0.25849223136901855, 0.2448681890964508, 0.2560148537158966, 0.2507774531841278, 0.249876469373703, 0.25224101543426514, 0.24522174894809723, 0.24906370043754578, 0.23802116513252258, 0.2480960637331009, 0.24006381630897522, 0.23467010259628296, 0.24838599562644958, 0.2339630126953125, 0.2327413558959961, 0.23907972872257233, 0.24464483559131622, 0.2300810068845749, 0.23509134352207184, 0.23316480219364166, 0.23055502772331238, 0.2514953017234802, 0.23392151296138763, 0.23511342704296112, 0.23399017751216888, 0.23956748843193054, 0.2443096786737442, 0.25080010294914246, 0.24242529273033142, 0.23616430163383484, 0.21867229044437408, 0.23714420199394226, 0.2222222089767456, 0.2265682816505432, 0.2537190318107605, 0.22757354378700256, 0.227090984582901, 0.23991672694683075, 0.22541439533233643, 0.22718968987464905, 0.2156842201948166, 0.23786534368991852, 0.2181944102048874, 0.23997803032398224, 0.24003826081752777, 0.2311122864484787, 0.21321998536586761, 0.23638828098773956, 0.23136372864246368, 0.22994908690452576, 0.24023301899433136, 0.19902457296848297, 0.23230643570423126, 0.24136152863502502, 0.24359773099422455, 0.20996686816215515, 0.23605823516845703, 0.21620793640613556, 0.23399755358695984, 0.24146471917629242, 0.25086796283721924, 0.2273932844400406, 0.2486996352672577, 0.2305045872926712, 0.2285119593143463, 0.25671499967575073, 0.23905625939369202, 0.24242611229419708, 0.24110369384288788, 0.2372860461473465, 0.22663383185863495, 0.24679706990718842, 0.27216318249702454, 0.23820844292640686, 0.24909883737564087, 0.24484284222126007, 0.22801746428012848, 0.23913541436195374, 0.24423415958881378, 0.2818070352077484, 0.23418834805488586, 0.24099136888980865, 0.24646684527397156, 0.2155550867319107, 0.2638334333896637, 0.28969377279281616, 0.2592369318008423, 0.29257598519325256, 0.2718951404094696, 0.21873843669891357, 0.24747689068317413, 0.2950564920902252, 0.24315184354782104, 0.2356022149324417, 0.28571817278862, 0.22756986320018768, 0.3056158721446991, 0.2407774031162262, 0.25483155250549316, 0.2552085220813751, 0.22644661366939545, 0.25831732153892517, 0.2866068482398987, 0.2718980312347412, 0.2526355981826782, 0.24708148837089539, 0.27744239568710327, 0.25174373388290405, 0.2670179009437561, 0.2606807053089142, 0.2715649902820587, 0.24789391458034515, 0.2814311981201172, 0.27268531918525696, 0.25306329131126404, 0.23911544680595398, 0.29383131861686707, 0.27863889932632446, 0.26037847995758057, 0.26618582010269165, 0.27518129348754883, 0.29372575879096985, 0.2721810042858124, 0.2491871863603592, 0.27739083766937256, 0.2495182305574417, 0.2982403337955475, 0.2981718182563782, 0.2576833665370941, 0.3366398215293884, 0.2623470723628998, 0.29957887530326843, 0.2753056585788727, 0.27555158734321594, 0.3031189441680908, 0.263321191072464, 0.2508396506309509, 0.25930312275886536, 0.3368796408176422, 0.31122952699661255, 0.2639197111129761, 0.3380315899848938, 0.2643410861492157, 0.29075926542282104, 0.28894540667533875, 0.2676645517349243, 0.292224645614624, 0.3692350685596466, 0.2698048949241638, 0.2700076401233673, 0.3211764991283417, 0.26851382851600647, 0.3200891315937042, 0.2810896933078766, 0.31205153465270996, 0.29149577021598816, 0.3175801932811737, 0.2761518657207489, 0.30915260314941406, 0.33102723956108093, 0.3235587477684021, 0.2336193174123764, 0.25501635670661926, 0.36986082792282104, 0.32773175835609436, 0.29854801297187805, 0.3313645124435425, 0.3699174225330353, 0.31849467754364014, 0.3393248915672302, 0.31387433409690857, 0.2853580415248871, 0.3334503173828125, 0.3129778504371643, 0.3160209059715271, 0.3185029625892639, 0.33792397379875183, 0.35739603638648987, 0.3835711181163788]}, {\"mode\": \"lines\", \"name\": \"Validation loss\", \"type\": \"scatter\", \"x\": [4.0, 5.959183673469388, 7.918367346938775, 9.877551020408163, 11.83673469387755, 13.795918367346939, 15.755102040816325, 17.714285714285715, 19.6734693877551, 21.632653061224488, 23.591836734693878, 25.551020408163264, 27.51020408163265, 29.46938775510204, 31.428571428571427, 33.38775510204081, 35.3469387755102, 37.30612244897959, 39.265306122448976, 41.224489795918366, 43.183673469387756, 45.14285714285714, 47.10204081632653, 49.06122448979592, 51.0204081632653, 52.97959183673469, 54.93877551020408, 56.89795918367347, 58.857142857142854, 60.816326530612244, 62.775510204081634, 64.73469387755102, 66.6938775510204, 68.65306122448979, 70.61224489795919, 72.57142857142857, 74.53061224489795, 76.48979591836735, 78.44897959183673, 80.40816326530611, 82.36734693877551, 84.3265306122449, 86.28571428571428, 88.24489795918367, 90.20408163265306, 92.16326530612244, 94.12244897959184, 96.08163265306122, 98.0408163265306, 100.0], \"y\": [0.2916125953197479, 0.29206401109695435, 0.2649984657764435, 0.31063956022262573, 0.28970980644226074, 0.26786282658576965, 0.2885185480117798, 0.30313214659690857, 0.2938250005245209, 0.3428782522678375, 0.3035199046134949, 0.32733389735221863, 0.3181736171245575, 0.3175463378429413, 0.35970282554626465, 0.35160204768180847, 0.3122960329055786, 0.3630715608596802, 0.33149081468582153, 0.3474027216434479, 0.33341577649116516, 0.330265611410141, 0.36859601736068726, 0.37672847509384155, 0.3810259699821472, 0.38657674193382263, 0.4179037809371948, 0.3866874873638153, 0.36013829708099365, 0.39903122186660767, 0.35158395767211914, 0.39871349930763245, 0.3915027976036072, 0.4100562632083893, 0.4240829348564148, 0.4959541857242584, 0.38752156496047974, 0.4595920443534851, 0.45008769631385803, 0.4272483289241791, 0.4347655773162842, 0.4796096086502075, 0.49062278866767883, 0.5512747764587402, 0.45909562706947327, 0.5228056311607361, 0.5470994114875793, 0.5866376757621765, 0.48789501190185547, 0.6066125631332397, 0.5214676260948181, 0.48828840255737305, 0.5706855654716492, 0.5092463493347168, 0.5305899977684021, 0.4823257327079773, 0.5886409282684326, 0.5574861764907837, 0.5393264889717102, 0.55743408203125, 0.5978662371635437, 0.48924267292022705, 0.6022181510925293, 0.6103795766830444, 0.5874599814414978, 0.5734784603118896, 0.5836281180381775, 0.6829484701156616, 0.5295451283454895, 0.5412919521331787, 0.5875069499015808, 0.6925822496414185, 0.7112658619880676, 0.8139869570732117, 0.6595863103866577, 0.6827331781387329, 0.7589082717895508, 0.6908050179481506, 0.6215704083442688, 0.8284938335418701, 0.8325817584991455, 0.9396462440490723, 0.754442572593689, 0.7276094555854797, 0.8788909316062927, 0.896244466304779, 0.7937697768211365, 0.7596790194511414, 0.6837592124938965, 0.8021548390388489, 0.6454310417175293, 0.7915502786636353, 0.6653414964675903, 0.6793502569198608, 0.7005038857460022, 0.5933985710144043, 0.862946629524231, 0.8503959774971008, 0.6684399247169495, 0.776265561580658, 0.7601169347763062, 0.5813968181610107, 0.5851815938949585, 0.5753586292266846, 0.7981837391853333, 0.708848774433136, 0.9233517050743103, 0.7431418299674988, 0.7230045795440674, 0.7770428657531738, 0.7196511030197144, 0.7771782279014587, 0.5821855068206787, 0.7246357202529907, 0.7651837468147278, 0.6928203105926514, 1.0022695064544678, 0.872398853302002, 0.9425690174102783, 0.7099226713180542, 1.0135695934295654, 0.5532710552215576, 0.8536514639854431, 0.7355425357818604, 0.9193890690803528, 0.6256332397460938, 0.84927898645401, 0.553149402141571, 0.6668237447738647, 0.8498729467391968, 0.7109194993972778, 0.9134702682495117, 0.6998801827430725, 0.6069531440734863, 1.0795546770095825, 0.9695292711257935, 0.8000343441963196, 0.5487595200538635, 0.9116407632827759, 1.0355417728424072, 0.7298080921173096, 0.7691970467567444, 0.87070232629776, 0.9988784790039062, 1.6172653436660767, 0.7322505712509155, 0.7370355725288391, 0.7563067078590393, 0.8395065665245056, 0.8496031165122986, 0.9134842753410339, 0.7576696872711182, 1.0219485759735107, 0.6246840357780457, 0.8988015055656433, 0.9541135430335999, 1.112856388092041, 0.9137162566184998, 0.7718033194541931, 0.6700653433799744, 1.1441959142684937, 0.8620131611824036, 0.8988513350486755, 0.843489408493042, 0.9770057201385498, 0.7821421027183533, 0.9869025349617004, 1.1031930446624756, 0.8527582287788391, 1.6674737930297852, 0.6499578356742859, 0.6501103043556213, 0.9084962010383606, 0.6566777229309082, 1.0210329294204712, 0.7801650166511536, 0.6435989141464233, 0.5221112966537476, 0.6734175682067871, 1.0503175258636475, 0.8557054996490479, 0.8323059678077698, 1.0057097673416138, 0.8723182082176208, 0.7200339436531067, 0.9766197800636292, 1.0887333154678345, 1.08988618850708, 0.6373622417449951, 0.7946556210517883, 0.6500340104103088, 0.7296397089958191, 1.031095266342163, 0.7645733952522278, 0.8613528609275818, 0.8727497458457947, 0.7661334872245789, 0.49651196599006653, 0.7522651553153992, 0.7063005566596985, 0.8769248127937317, 0.6787411570549011, 1.0348678827285767]}],\n",
              "                        {\"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('17b4b25e-99f6-4195-baf8-32be9a42e25b');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qOH3TKEYQnl",
        "outputId": "0600da56-bc1d-4fde-fcb6-a04cb2c9c3ea"
      },
      "source": [
        "# evaluate the model\n",
        "\n",
        "train_acc = model.evaluate(train_images,train_labels, verbose=0)\n",
        "test_acc = model.evaluate(test_images,test_labels, verbose=0)\n",
        "print(\"Training Performance:\",dict(zip(model.metrics_names, train_acc)))\n",
        "print(\"Testing performance:\",dict(zip(model.metrics_names, test_acc)))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Performance: {'loss': 0.1709606647491455, 'accuracy': 0.9675499796867371}\n",
            "Testing performance: {'loss': 1.1751949787139893, 'accuracy': 0.9111999869346619}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCIsg5NJIXWY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}